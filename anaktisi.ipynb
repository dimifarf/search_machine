{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335645a2-caa8-4944-bd6e-0317e4d00231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\difi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\difi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\difi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What Are You Looking For? cat and dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\difi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter The Query:  cat and dog\n",
      "\n",
      "What Type Of Retrieval You Want To Use? \n",
      "1)Boolean Retrieval \n",
      "2)Vector Space Model \n",
      "3)Okapi BM25\n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Results:\n",
      "Document 1:\n",
      "Title: Image Classification Using Singular Value Decomposition and Optimization\n",
      "Authors: Authors: Isabela M. Yepes , Manasvi Goyal\n",
      "Abstract: This study investigates the applicability of Singular Value Decomposition for the image classification of specific breeds of cats and dogs using fur color as the primary identifying feature. Sequential Quadratic Programming (SQP) is employed to construct optimally weighted templates. The proposed method achieves 69% accuracy using the Frobenius norm at rank 10. The results partially validate the assumption that dominant features, such as fur color, can be effectively captured through low-rank approximations. However, the accuracy suggests that additional features or methods may be required for more robust classification, highlighting the trade-off between simplicity and performance in resource-constrained environments. △ Less\n",
      "Sumbitted: 10/12/2024\n",
      "\n",
      "\n",
      "Document 3:\n",
      "Title: Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos\n",
      "Authors: Authors: Gengshan Yang , Andrea Bajcsy , Shunsuke Saito , Angjoo Kanazawa\n",
      "Abstract: We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat , dog , bunny) and human given monocular RGBD videos captured by a smartphone. △ Less\n",
      "Sumbitted: 21/10/2024\n",
      "\n",
      "\n",
      "Document 4:\n",
      "Title: Inference and Verbalization Functions During In-Context Learning\n",
      "Authors: Authors: Junyi Tao , Xiaoyin Chen , Nelson F. Liu\n",
      "Abstract: Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., \"true\"/\"false\" to \" cat \"/\" dog \"), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B. △ Less\n",
      "Sumbitted: 11/10/2024\n",
      "\n",
      "\n",
      "Document 5:\n",
      "Title: A Cat Is A Cat (Not A Dog !): Unraveling Information Mix-ups in Text-to-Image Encoders through Causal Analysis and Embedding Optimization\n",
      "Authors: Authors: Chieh-Yun Chen , Chiang Tseng , Li-Wu Tsao , Hong-Han Shuai\n",
      "Abstract: This paper analyzes the impact of causal manner in the text encoder of text-to-image (T2I) diffusion models, which can lead to information bias and loss. Previous works have focused on addressing the issues through the denoising process. However, there is no research discussing how text embedding contributes to T2I models, especially when generating more than one object. In this paper, we share a comprehensive analysis of text embedding: i) how text embedding contributes to the generated images and ii) why information gets lost and biases towards the first-mentioned object. Accordingly, we propose a simple but effective text embedding balance optimization method, which is training-free, with an improvement of 125.42% on information balance in stable diffusion. Furthermore, we propose a new automatic evaluation metric that quantifies information loss more accurately than existing methods, achieving 81% concordance with human assessments. This metric effectively measures the presence and accuracy of objects, addressing the limitations of current distribution scores like CLIP's text-image similarities. △ Less\n",
      "Sumbitted: 07/11/2024\n",
      "\n",
      "\n",
      "Document 6:\n",
      "Title: Playful DoggyBot: Learning Agile and Precise Quadrupedal Locomotion\n",
      "Authors: Authors: Xin Duan , Ziwen Zhuang , Hang Zhao , Soeren Schwertfeger\n",
      "Abstract: Quadrupedal animals have the ability to perform agile while accurate tasks: a trained dog can chase and catch a flying frisbee before it touches the ground; a cat alone at home can jump and grab the door handle accurately. However, agility and precision are usually a trade-off in robotics problems. Recent works in quadruped robots either focus on agile but not-so-accurate tasks, such as locomotion in challenging terrain, or accurate but not-so-fast tasks, such as using an additional manipulator to interact with objects. In this work, we aim at an accurate and agile task, catching a small object hanging above the robot. We mount a passive gripper in front of the robot chassis, so that the robot has to jump and catch the object with extreme precision. Our experiment shows that our system is able to jump and successfully catch the ball at 1.05m high in simulation and 0.8m high in the real world, while the robot is 0.3m high when standing. △ Less\n",
      "Sumbitted: 11/11/2024\n",
      "\n",
      "\n",
      "Document 7:\n",
      "Title: Commonly Interesting Images\n",
      "Authors: Authors: Fitim Abdullahu , Helmut Grabner\n",
      "Abstract: Images tell stories, trigger emotions, and let us recall memories -- they make us think. Thus, they have the ability to attract and hold one's attention, which is the definition of being \"interesting\". Yet, the appeal of an image is highly subjective. Looking at the image of my son taking his first steps will always bring me back to this emotional moment, while it is just a blurry, quickly taken snapshot to most others. Preferences vary widely: some adore cats , others are dog enthusiasts, and a third group may not be fond of either. We argue that every image can be interesting to a particular observer under certain circumstances. This work particularly emphasizes subjective preferences. However, our analysis of 2.5k image collections from diverse users of the photo-sharing platform Flickr reveals that specific image characteristics make them commonly more interesting. For instance, images, including professionally taken landscapes, appeal broadly due to their aesthetic qualities. In contrast, subjectively interesting images, such as those depicting personal or niche community events, resonate on a more individual level, often evoking personal memories and emotions. △ Less\n",
      "Sumbitted: 25/09/2024\n",
      "\n",
      "\n",
      "Document 8:\n",
      "Title: How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?\n",
      "Authors: Authors: Huaizhi Ge , Frank Rudzicz , Zining Zhu\n",
      "Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but updating their knowledge post-training remains a critical challenge. While recent model editing techniques like Rank-One Model Editing (ROME) show promise, their effectiveness may vary based on the nature of the knowledge being edited. We introduce the concept of ``perplexingness'': the degree to which new knowledge conflicts with an LLM's learned conceptual hierarchies and categorical relationships. For instance, editing ``British Shorthair is a kind of cat '' to ``British Shorthair is a kind of dog '' represents a low-perplexingness edit within the same taxonomic level, while editing ``A cat is a kind of animal'' to ``A cat is a kind of plant'' represents a high-perplexingness edit that violates fundamental categorical boundaries. To systematically investigate this phenomenon, we introduce HierarchyData, a carefully curated dataset of 99 hyponym-hypernym pairs across diverse categories. Through controlled experiments across three models and four editing methods, we demonstrate a strong negative correlation between the perplexingness of new knowledge and the effectiveness of knowledge editing. Our analysis reveals that edits involving more abstract concepts (hypernyms) generally exhibit higher perplexingness and are more resistant to modification than their specific counterparts (hyponyms). These findings highlight a fundamental challenge in LLM knowledge editing: the more a new fact contradicts an LLM's learned conceptual hierarchies, the harder it becomes to reliably encode that knowledge. △ Less\n",
      "Sumbitted: 16/12/2024\n",
      "\n",
      "\n",
      "Document 9:\n",
      "Title: DISentangled Counterfactual Visual interpretER (DISCOVER) generalizes to natural images\n",
      "Authors: Authors: Oded Rotem , Assaf Zaritsky\n",
      "Abstract: We recently presented DISentangled COunterfactual Visual interpretER (DISCOVER), a method toward systematic visual interpretability of image-based classification models and demonstrated its applicability to two biomedical domains. Here we demonstrate that DISCOVER can be applied to the domain of natural images. First, DISCOVER visually interpreted the nose size, the muzzle area, and the face size as semantic discriminative visual traits discriminating between facial images of dogs versus cats . Second, DISCOVER visually interpreted the cheeks and jawline, eyebrows and hair, and the eyes, as discriminative facial characteristics. These successful visual interpretations across two natural images domains indicate that DISCOVER is a generalized interpretability method. △ Less\n",
      "Sumbitted: 22/06/2024\n",
      "\n",
      "\n",
      "Document 11:\n",
      "Title: Yo'LLaVA: Your Personalized Language and Vision Assistant\n",
      "Authors: Authors: Thao Nguyen , Haotian Liu , Yuheng Li , Mu Cai , Utkarsh Ojha , Yong Jae Lee\n",
      "Abstract: Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering). While broad, their knowledge remains generic (e.g., recognizing a dog ), and they are unable to handle personalized subjects (e.g., recognizing a user's pet dog ). Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, \"What should I buy for my dog's birthday?\"; as opposed to a generic inquiry about \"What should I buy for a dog's birthday?\". Similarly, when looking at a friend's image, the interest lies in seeing their activities (e.g., \"my friend is holding a cat \"), rather than merely observing generic human actions (e.g., \"a man is holding a cat \"). In this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject. Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA). △ Less\n",
      "Sumbitted: 04/12/2024\n",
      "\n",
      "\n",
      "Document 13:\n",
      "Title: Training-free Editioning of Text-to-Image Models\n",
      "Authors: Authors: Jinqi Wang , Yunfei Fu , Zhangcan Ding , Bailin Deng , Yu-Kun Lai , Yipeng Qin\n",
      "Abstract: Inspired by the software industry's practice of offering different editions or versions of a product tailored to specific user groups or use cases, we propose a novel task, namely, training-free editioning, for text-to-image models. Specifically, we aim to create variations of a base text-to-image model without retraining, enabling the model to cater to the diverse needs of different user groups or to offer distinct features and functionalities. To achieve this, we propose that different editions of a given text-to-image model can be formulated as concept subspaces in the latent space of its text encoder (e.g., CLIP). In such a concept subspace, all points satisfy a specific user need (e.g., generating images of a cat lying on the grass/ground/falling leaves). Technically, we apply Principal Component Analysis (PCA) to obtain the desired concept subspaces from representative text embedding that correspond to a specific user need or requirement. Projecting the text embedding of a given prompt into these low-dimensional subspaces enables efficient model editioning without retraining. Intuitively, our proposed editioning paradigm enables a service provider to customize the base model into its \" cat edition\" (or other editions) that restricts image generation to cats , regardless of the user's prompt (e.g., dogs , people, etc.). This introduces a new dimension for product differentiation, targeted functionality, and pricing strategies, unlocking novel business models for text-to-image generators. Extensive experimental results demonstrate the validity of our approach and its potential to enable a wide range of customized text-to-image model editions across various domains and applications. △ Less\n",
      "Sumbitted: 27/05/2024\n",
      "\n",
      "\n",
      "Document 14:\n",
      "Title: Automated Optimal Layout Generator for Animal Shelters: A framework based on Genetic Algorithm, TOPSIS and Graph Theory\n",
      "Authors: Authors: Arghavan Jalayer , Masoud Jalayer , Mehdi Khakzand , Mohsen Faizi\n",
      "Abstract: Overpopulation in animal shelters contributes to increased disease spread and higher expenses on animal healthcare, leading to fewer adoptions and more shelter deaths. Additionally, one of the greatest challenges that shelters face is the noise level in the dog kennel area, which is physically and physiologically hazardous for both animals and staff. This paper proposes a multi-criteria optimization framework to automatically design cage layouts that maximize shelter capacity, minimize tension in the dog kennel area by reducing the number of cages facing each other, and ensure accessibility for staff and visitors. The proposed framework uses a Genetic Algorithm (GA) to systematically generate and improve layouts. A novel graph theory-based algorithm is introduced to process solutions and calculate fitness values. Additionally, the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) is used to rank and sort the layouts in each iteration. The graph-based algorithm calculates variables such as cage accessibility and shortest paths to access points. Furthermore, a heuristic algorithm is developed to calculate layout scores based on the number of cages facing each other. This framework provides animal shelter management with a flexible decision-support system that allows for different strategies by assigning various weights to the TOPSIS criteria. Results from cats ' and dogs ' kennel areas show that the proposed framework can suggest optimal layouts that respect different priorities within acceptable runtimes. △ Less\n",
      "Sumbitted: 29/05/2024\n",
      "\n",
      "\n",
      "Document 15:\n",
      "Title: DogFLW: Dog Facial Landmarks in the Wild Dataset\n",
      "Authors: Authors: George Martvel , Greta Abele , Annika Bremhorst , Chiara Canori , Nareed Farhat , Giulia Pedretti , Ilan Shimshoni , Anna Zamansky\n",
      "Abstract: Affective computing for animals is a rapidly expanding research area that is going deeper than automated movement tracking to address animal internal states, like pain and emotions. Facial expressions can serve to communicate information about these states in mammals. However, unlike human-related studies, there is a significant shortage of datasets that would enable the automated analysis of animal facial expressions. Inspired by the recently introduced Cat Facial Landmarks in the Wild dataset, presenting cat faces annotated with 48 facial anatomy-based landmarks, in this paper, we develop an analogous dataset containing 3,274 annotated images of dogs . Our dataset is based on a scheme of 46 facial anatomy-based landmarks. The DogFLW dataset is available from the corresponding author upon a reasonable request. △ Less\n",
      "Sumbitted: 19/05/2024\n",
      "\n",
      "\n",
      "Document 17:\n",
      "Title: CLoRA: A Contrastive Approach to Compose Multiple LoRA Models\n",
      "Authors: Authors: Tuna Han Salih Meral , Enis Simsar , Federico Tombari , Pinar Yanardag\n",
      "Abstract: Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique in the field of image generation, offering a highly effective way to adapt and refine pre-trained deep learning models for specific tasks without the need for comprehensive retraining. By employing pre-trained LoRA models, such as those representing a specific cat and a particular dog , the objective is to generate an image that faithfully embodies both animals as defined by the LoRAs. However, the task of seamlessly blending multiple concept LoRAs to capture a variety of concepts in one image proves to be a significant challenge. Common approaches often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog ) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog ). To overcome these issues, CLoRA addresses them by updating the attention maps of multiple LoRA models and leveraging them to create semantic masks that facilitate the fusion of latent representations. Our method enables the creation of composite images that truly reflect the characteristics of each LoRA, successfully merging multiple concepts or styles. Our comprehensive evaluations, both qualitative and quantitative, demonstrate that our approach outperforms existing methodologies, marking a significant advancement in the field of image generation with LoRAs. Furthermore, we share our source code, benchmark dataset, and trained LoRA models to promote further research on this topic. △ Less\n",
      "Sumbitted: 28/03/2024\n",
      "\n",
      "\n",
      "Document 18:\n",
      "Title: Matching Non-Identical Objects\n",
      "Authors: Authors: Yusuke Marumo , Kazuhiko Kawamoto , Satomi Tanaka , Shigenobu Hirano , Hiroshi Kera\n",
      "Abstract: Not identical but similar objects are ubiquitous in our world, ranging from four-legged animals such as dogs and cats to cars of different models and flowers of various colors. This study addresses a novel task of matching such non-identical objects at the pixel level. We propose a weighting scheme of descriptors that incorporates semantic information from object detectors into existing sparse image matching methods, extending their targets from identical objects captured from different perspectives to semantically similar objects. The experiments show successful matching between non-identical objects in various cases, including in-class design variations, class discrepancy, and domain shifts (e.g., photo--drawing and image corruptions). △ Less\n",
      "Sumbitted: 09/12/2024\n",
      "\n",
      "\n",
      "Document 19:\n",
      "Title: Pretraining and the Lasso\n",
      "Authors: Authors: Erin Craig , Mert Pilanci , Thomas Le Menestrel , Balasubramanian Narasimhan , Manuel Rivas , Stein-Erik Gullaksen , Roozbeh Dehghannasiri , Julia Salzman , Jonathan Taylor , Robert Tibshirani\n",
      "Abstract: Pretraining is a popular and powerful paradigm in machine learning to pass information from one model to another. As an example, suppose one has a modest-sized dataset of images of cats and dogs , and plans to fit a deep neural network to classify them from the pixel features. With pretraining, we start with a neural network trained on a large corpus of images, consisting of not just cats and dogs but hundreds of other image types. Then we fix all of the network weights except for the top layer(s) (which makes the final classification) and train (or \"fine tune\") those weights on our dataset. This often results in dramatically better performance than the network trained solely on our smaller dataset. In this paper, we ask the question \"Can pretraining help the lasso?\".\n",
      "  We develop a framework for the lasso in which a model is fit to a large dataset, and then fine-tuned using a smaller dataset. This latter dataset can be a subset of the original dataset, or it can be a dataset with a different but related outcome. This framework has a wide variety of applications, including stratified models, multinomial responses, multi-response models, conditional average treatment estimation and even gradient boosting.\n",
      "  In the stratified model setting, the pretrained lasso pipeline estimates the coefficients common to all groups at the first stage, and then estimates the group-specific coefficients at the second \"fine-tuning\" stage. We show that under appropriate assumptions, the support recovery rate of the common coefficients is superior to that of the usual lasso trained only on individual groups. This separate identification of common and individual coefficients can also be useful for scientific understanding. △ Less\n",
      "Sumbitted: 29/10/2024\n",
      "\n",
      "\n",
      "Document 21:\n",
      "Title: On the Stability of a non-hyperbolic nonlinear map with non-bounded set of non-isolated fixed points with applications to Machine Learning\n",
      "Authors: Authors: Roberta Hansen , Matias Vera , Lautaro Estienne , Luciana Ferrer , Pablo Piantanida\n",
      "Abstract: This paper deals with the convergence analysis of the SUCPA (Semi Unsupervised Calibration through Prior Adaptation) algorithm, defined from a first-order non-linear difference equations, first developed to correct the scores output by a supervised machine learning classifier. The convergence analysis is addressed as a dynamical system problem, by studying the local and global stability of the nonlinear map derived from the algorithm. This map, which is defined by a composition of exponential and rational functions, turns out to be non-hyperbolic with a non-bounded set of non-isolated fixed points. Hence, a non-standard method for solving the convergence analysis is used consisting of an ad-hoc geometrical approach. For a binary classification problem (two-dimensional map), we rigorously prove that the map is globally asymptotically stable. Numerical experiments on real-world application are performed to support the theoretical results by means of two different classification problems: Sentiment Polarity performed with a Large Language Model and Cat - Dog Image classification. For a greater number of classes, the numerical evidence shows the same behavior of the algorithm, and this is illustrated with a Natural Language Inference example. The experiment codes are publicly accessible online at the following repository: https://github.com/LautaroEst/sucpa-convergence △ Less\n",
      "Sumbitted: 25/04/2024\n",
      "\n",
      "\n",
      "Document 23:\n",
      "Title: A proposed new metric for the conceptual diversity of a text\n",
      "Authors: Authors: İlknur Dönmez Phd , Mehmet Haklıdır Phd\n",
      "Abstract: A word may contain one or more hidden concepts. While the \"animal\" word evokes many images in our minds and encapsulates many concepts (birds, dogs , cats , crocodiles, etc.), the `parrot' word evokes a single image (a colored bird with a short, hooked beak and the ability to mimic sounds). In spoken or written texts, we use some words in a general sense and some in a detailed way to point to a specific object. Until now, a text's conceptual diversity value cannot be determined using a standard and precise technique. This research contributes to the natural language processing field of AI by offering a standardized method and a generic metric for evaluating and comparing concept diversity in different texts and domains. It also contributes to the field of semantic research of languages. If we give examples for the diversity score of two sentences, \"He discovered an unknown entity.\" has a high conceptual diversity score (16.6801), and \"The endoplasmic reticulum forms a series of flattened sacs within the cytoplasm of eukaryotic cells.\" sentence has a low conceptual diversity score which is 3.9068. △ Less\n",
      "Sumbitted: 27/12/2023\n",
      "\n",
      "\n",
      "Document 24:\n",
      "Title: QuadAttack: A Quadratic Programming Approach to Ordered Top-K Attacks\n",
      "Authors: Authors: Thomas Paniagua , Ryan Grainger , Tianfu Wu\n",
      "Abstract: The adversarial vulnerability of Deep Neural Networks (DNNs) has been well-known and widely concerned, often under the context of learning top-$1$ attacks (e.g., fooling a DNN to classify a cat image as dog ). This paper shows that the concern is much more serious by learning significantly more aggressive ordered top-$K$ clear-box~\\footnote{ This is often referred to as white/black-box attacks in the literature. We choose to adopt neutral terminology, clear/opaque-box attacks in this paper, and omit the prefix clear-box for simplicity.} targeted attacks proposed in Adversarial Distillation. We propose a novel and rigorous quadratic programming (QP) method of learning ordered top-$K$ attacks with low computing cost, dubbed as \\textbf{QuadAttac$K$}. Our QuadAttac$K$ directly solves the QP to satisfy the attack constraint in the feature embedding space (i.e., the input space to the final linear classifier), which thus exploits the semantics of the feature embedding space (i.e., the principle of class coherence). With the optimized feature embedding vector perturbation, it then computes the adversarial perturbation in the data space via the vanilla one-step back-propagation. In experiments, the proposed QuadAttac$K$ is tested in the ImageNet-1k classification using ResNet-50, DenseNet-121, and Vision Transformers (ViT-B and DEiT-S). It successfully pushes the boundary of successful ordered top-$K$ attacks from $K=10$ up to $K=20$ at a cheap budget ($1\\times 60$) and further improves attack success rates for $K=5$ for all tested models, while retaining the performance for $K=1$. △ Less\n",
      "Sumbitted: 12/12/2023\n",
      "\n",
      "\n",
      "Document 25:\n",
      "Title: Impact of Data Augmentation on QCNNs\n",
      "Authors: Authors: Leting Zhouli , Peiyong Wang , Udaya Parampalli\n",
      "Abstract: In recent years, Classical Convolutional Neural Networks (CNNs) have been applied for image recognition successfully. Quantum Convolutional Neural Networks (QCNNs) are proposed as a novel generalization to CNNs by using quantum mechanisms. The quantum mechanisms lead to an efficient training process in QCNNs by reducing the size of input from $N$ to $log_2N$. This paper implements and compares both CNNs and QCNNs by testing losses and prediction accuracy on three commonly used datasets. The datasets include the MNIST hand-written digits, Fashion MNIST and cat / dog face images. Additionally, data augmentation (DA), a technique commonly used in CNNs to improve the performance of classification by generating similar images based on original inputs, is also implemented in QCNNs. Surprisingly, the results showed that data augmentation didn't improve QCNNs performance. The reasons and logic behind this result are discussed, hoping to expand our understanding of Quantum machine learning theory. △ Less\n",
      "Sumbitted: 01/12/2023\n",
      "\n",
      "\n",
      "Document 26:\n",
      "Title: Unlocking Spatial Comprehension in Text-to-Image Diffusion Models\n",
      "Authors: Authors: Mohammad Mahdi Derakhshani , Menglin Xia , Harkirat Behl , Cees G. M. Snoek , Victor Rühle\n",
      "Abstract: We propose CompFuser, an image generation pipeline that enhances spatial comprehension and attribute assignment in text-to-image generative models. Our pipeline enables the interpretation of instructions defining spatial relationships between objects in a scene, such as `An image of a gray cat on the left of an orange dog ', and generate corresponding images. This is especially important in order to provide more control to the user. CompFuser overcomes the limitation of existing text-to-image diffusion models by decoding the generation of multiple objects into iterative steps: first generating a single object and then editing the image by placing additional objects in their designated positions. To create training data for spatial comprehension and attribute assignment we introduce a synthetic data generation process, that leverages a frozen large language model and a frozen layout-based diffusion model for object placement. We compare our approach to strong baselines and show that our model outperforms state-of-the-art image generation models in spatial comprehension and attribute assignment, despite being 3x to 5x smaller in parameters. △ Less\n",
      "Sumbitted: 28/11/2023\n",
      "\n",
      "\n",
      "Document 27:\n",
      "Title: Hierarchical Simplicity Bias of Neural Networks\n",
      "Authors: Authors: Zhehang Du\n",
      "Abstract: Neural networks often exhibit simplicity bias, favoring simpler features over more complex ones, even when both are equally predictive. We introduce a novel method called imbalanced label coupling to explore and extend this simplicity bias across multiple hierarchical levels. Our approach demonstrates that trained networks sequentially consider features of increasing complexity based on their correlation with labels in the training set, regardless of their actual predictive power. For example, in CIFAR-10, simple spurious features can cause misclassifications where most cats are predicted as dogs and most trucks as automobiles. We empirically show that last-layer retraining with target data distribution \\citep{kirichenko2022last} is insufficient to fully recover core features when spurious features perfectly correlate with target labels in our synthetic datasets. Our findings deepen the understanding of the implicit biases inherent in neural networks. △ Less\n",
      "Sumbitted: 21/10/2024\n",
      "\n",
      "\n",
      "Document 28:\n",
      "Title: Attribute Based Interpretable Evaluation Metrics for Generative Models\n",
      "Authors: Authors: Dongkyun Kim , Mingi Kwon , Youngjung Uh\n",
      "Abstract: When the training dataset comprises a 1:1 proportion of dogs to cats , a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats . Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond \"diversity\". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we reveal the following about existing generative models. ProjectedGAN generates implausible attribute relationships such as a baby with a beard even though it has competitive scores of existing metrics. Diffusion models struggle to capture diverse colors in the datasets. The larger sampling timesteps of latent diffusion model generate the more minor objects including earrings and necklaces. Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models. △ Less\n",
      "Sumbitted: 17/07/2024\n",
      "\n",
      "\n",
      "Document 29:\n",
      "Title: Dynamics and frictional dissipation from treading in the puddle\n",
      "Authors: Authors: Chung-Hao Chen , Zong-Rou Jiang , Tzay-Ming Hong\n",
      "Abstract: It was recently established that dogs share the same lapping technique as cats by flicking their tongue against the water surface and then yanking it back, dragging up a column of water. This liquid column appears frequently in daily life and industrial applications, such as walking through a puddle and roller printing. While governed by the Navier-Stokes equation, its dynamics are often studied by numerical means, which hinders a full understanding of the rich mixture of physics behind, for instance, the competition of surface and potential energies, and how the pinch-off is affected by the kinetic energy and water jet when a large cylinder is used. Combined with simple models, we elucidate the mechanism that drives the change of morphology and derive analytic expressions for the critical height and upper radius for the liquid column when transiting between three stages. Stage I is characterized by a static and reversible profile for the column whose upper radius r_t equals that of the cylinder. The column becomes irreversible and $r_t$ starts shrinking upon entering stage II. It is not until r_t stops shrinking that the column neck accelerates its contraction and descends toward the pool, the quantitative behavior of which is among the successful predictions of our theory. Pinch-off dominates the second half of stage III without its usual signature of self-similarity. This is discussed and explained with an interesting incident involving a water jet similar to that made by a dropping stone. △ Less\n",
      "Sumbitted: 18/05/2024\n",
      "\n",
      "\n",
      "Document 31:\n",
      "Title: Blind Dates: Examining the Expression of Temporality in Historical Photographs\n",
      "Authors: Authors: Alexandra Barancová , Melvin Wevers , Nanne van Noord\n",
      "Abstract: This paper explores the capacity of computer vision models to discern temporal information in visual content, focusing specifically on historical photographs. We investigate the dating of images using OpenCLIP, an open-source implementation of CLIP, a multi-modal language and vision model. Our experiment consists of three steps: zero-shot classification, fine-tuning, and analysis of visual content. We use the \\textit{De Boer Scene Detection} dataset, containing 39,866 gray-scale historical press photographs from 1950 to 1999. The results show that zero-shot classification is relatively ineffective for image dating, with a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a logistic classifier improves performance and eliminates the bias. Additionally, our analysis reveals that images featuring buses, cars, cats , dogs , and people are more accurately dated, suggesting the presence of temporal markers. The study highlights the potential of machine learning models like OpenCLIP in dating images and emphasizes the importance of fine-tuning for accurate temporal analysis. Future research should explore the application of these findings to color photographs and diverse datasets. △ Less\n",
      "Sumbitted: 10/10/2023\n",
      "\n",
      "\n",
      "Document 32:\n",
      "Title: Divide & Bind Your Attention for Improved Generative Semantic Nursing\n",
      "Authors: Authors: Yumeng Li , Margret Keuper , Dan Zhang , Anna Khoreva\n",
      "Abstract: Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., \"a cat and a dog \". However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. △ Less\n",
      "Sumbitted: 14/07/2024\n",
      "\n",
      "\n",
      "Document 33:\n",
      "Title: Reconstructing Animatable Categories from Videos\n",
      "Authors: Authors: Gengshan Yang , Chaoyang Wang , N Dinesh Reddy , Deva Ramanan\n",
      "Abstract: Building animatable 3D models is challenging due to the need for 3D scans, laborious registration, and manual rigging, which are difficult to scale to arbitrary categories. Recently, differentiable rendering provides a pathway to obtain high-quality 3D models from monocular videos, but these are limited to rigid categories or single instances. We present RAC that builds category 3D models from monocular videos while disentangling variations over instances and motion over time. Three key ideas are introduced to solve this problem: (1) specializing a skeleton to instances via optimization, (2) a method for latent space regularization that encourages shared structure across a category while maintaining instance details, and (3) using 3D background models to disentangle objects from the background. We show that 3D models of humans, cats , and dogs can be learned from 50-100 internet videos. △ Less\n",
      "Sumbitted: 10/05/2023\n",
      "\n",
      "\n",
      "Document 34:\n",
      "Title: SLoMo: A General System for Legged Robot Motion Imitation from Casual Videos\n",
      "Authors: Authors: John Z. Zhang , Shuo Yang , Gengshan Yang , Arun L. Bishop , Deva Ramanan , Zachary Manchester\n",
      "Abstract: We present SLoMo: a first-of-its-kind framework for transferring skilled motions from casually captured \"in the wild\" video footage of humans and animals to legged robots. SLoMo works in three stages: 1) synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2) optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as contact sequences that closely tracks the key points; 3) track the reference trajectory online using a general-purpose model-predictive controller on robot hardware. Traditional motion imitation for legged motor skills often requires expert animators, collaborative demonstrations, and/or expensive motion capture equipment, all of which limits scalability. Instead, SLoMo only relies on easy-to-obtain monocular video footage, readily available in online repositories such as YouTube. It converts videos into motion primitives that can be executed reliably by real-world robots. We demonstrate our approach by transferring the motions of cats , dogs , and humans to example robots including a quadruped (on hardware) and a humanoid (in simulation). To the best knowledge of the authors, this is the first attempt at a general-purpose motion transfer framework that imitates animal and human motions on legged robots directly from casual videos without artificial markers or labels. △ Less\n",
      "Sumbitted: 05/09/2023\n",
      "\n",
      "\n",
      "Document 35:\n",
      "Title: Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning\n",
      "Authors: Authors: Ella Gale , Oliver Matthews\n",
      "Abstract: Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical learning based `mistakes' made by infants when attending to images of cats and dogs . 2. improving both the resilience to hacking images and the accurate measure of certainty in deep-NNs. 3. Relational reasoning over sets of images, including those not known to the NN nor seen before. We also demonstrate that ClusterFlow can work on non-NN data and deal with missing data by testing it on a Chemistry dataset. This work suggests that modern deep NNs can be made more human-like without re-training of the NNs. As it is known that some methods used in deep and convolutional NNs are not biologically plausible or perhaps even the best approach: the ClusterFlow framework can sit on top of any NN and will be a useful tool to add as NNs are improved in this regard. △ Less\n",
      "Sumbitted: 27/04/2023\n",
      "\n",
      "\n",
      "Document 36:\n",
      "Title: Equivariant Similarity for Vision-Language Foundation Models\n",
      "Authors: Authors: Tan Wang , Kevin Lin , Linjie Li , Chung-Ching Lin , Zhengyuan Yang , Hanwang Zhang , Zicheng Liu , Lijuan Wang\n",
      "Abstract: This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog , it is unclear to what extent the similarity changes when the pixel is changed from dog to cat ? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs and validate the effectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen. △ Less\n",
      "Sumbitted: 09/10/2023\n",
      "\n",
      "\n",
      "Document 37:\n",
      "Title: ESCAPE: Countering Systematic Errors from Machine's Blind Spots via Interactive Visual Analysis\n",
      "Authors: Authors: Yongsu Ahn , Yu-Ru Lin , Panpan Xu , Zeng Dai\n",
      "Abstract: Classification models learn to generalize the associations between data samples and their target classes. However, researchers have increasingly observed that machine learning practice easily leads to systematic errors in AI applications, a phenomenon referred to as AI blindspots. Such blindspots arise when a model is trained with training samples (e.g., cat / dog classification) where important patterns (e.g., black cats ) are missing or periphery/undesirable patterns (e.g., dogs with grass background) are misleading towards a certain class. Even more sophisticated techniques cannot guarantee to capture, reason about, and prevent the spurious associations. In this work, we propose ESCAPE, a visual analytic system that promotes a human-in-the-loop workflow for countering systematic errors. By allowing human users to easily inspect spurious associations, the system facilitates users to spontaneously recognize concepts associated misclassifications and evaluate mitigation strategies that can reduce biased associations. We also propose two statistical approaches, relative concept association to better quantify the associations between a concept and instances, and debias method to mitigate spurious associations. We demonstrate the utility of our proposed ESCAPE system and statistical measures through extensive evaluation including quantitative experiments, usage scenarios, expert interviews, and controlled user experiments. △ Less\n",
      "Sumbitted: 16/03/2023\n",
      "\n",
      "\n",
      "Document 38:\n",
      "Title: Demystifying What Code Summarization Models Learned\n",
      "Authors: Authors: Yu Wang , Ke Wang\n",
      "Abstract: Study patterns that models have learned has long been a focus of pattern recognition research. Explaining what patterns are discovered from training data, and how patterns are generalized to unseen data are instrumental to understanding and advancing the pattern recognition methods. Unfortunately, the vast majority of the application domains deal with continuous data (i.e. statistical in nature) out of which extracted patterns can not be formally defined. For example, in image classification, there does not exist a principle definition for a label of cat or dog . Even in natural language, the meaning of a word can vary with the context it is surrounded by. Unlike the aforementioned data format, programs are a unique data structure with a well-defined syntax and semantics, which creates a golden opportunity to formalize what models have learned from source code. This paper presents the first formal definition of patterns discovered by code summarization models (i.e. models that predict the name of a method given its body), and gives a sound algorithm to infer a context-free grammar (CFG) that formally describes the learned patterns.\n",
      "  We realize our approach in PATIC which produces CFGs for summarizing the patterns discovered by code summarization models. In particular, we pick two prominent instances, code2vec and code2seq, to evaluate PATIC. PATIC shows that the patterns extracted by each model are heavily restricted to local, and syntactic code structures with little to none semantic implication. Based on these findings, we present two example uses of the formal definition of patterns: a new method for evaluating the robustness and a new technique for improving the accuracy of code summarization models.\n",
      "  Our work opens up this exciting, new direction of studying what models have learned from source code. △ Less\n",
      "Sumbitted: 04/03/2023\n",
      "\n",
      "\n",
      "Document 39:\n",
      "Title: Neural Congealing: Aligning Images to a Joint Semantic Atlas\n",
      "Authors: Authors: Dolev Ofri-Amar , Michal Geyer , Yoni Kasten , Tali Dekel\n",
      "Abstract: We present Neural Congealing -- a zero-shot self-supervised framework for detecting and jointly aligning semantically-common content across a given set of images. Our approach harnesses the power of pre-trained DINO-ViT features to learn: (i) a joint semantic atlas -- a 2D grid that captures the mode of DINO-ViT features in the input set, and (ii) dense mappings from the unified atlas to each of the input images. We derive a new robust self-supervised framework that optimizes the atlas representation and mappings per image set, requiring only a few real-world images as input without any additional input information (e.g., segmentation masks). Notably, we design our losses and training paradigm to account only for the shared content under severe variations in appearance, pose, background clutter or other distracting objects. We demonstrate results on a plethora of challenging image sets including sets of mixed domains (e.g., aligning images depicting sculpture and artwork of cats ), sets depicting related yet different object categories (e.g., dogs and tigers), or domains for which large-scale training data is scarce (e.g., coffee mugs). We thoroughly evaluate our method and show that our test-time optimization approach performs favorably compared to a state-of-the-art method that requires extensive training on large-scale datasets. △ Less\n",
      "Sumbitted: 06/03/2023\n",
      "\n",
      "\n",
      "Document 40:\n",
      "Title: Deep leakage from gradients\n",
      "Authors: Authors: Yaqiong Mu\n",
      "Abstract: With the development of artificial intelligence technology, Federated Learning (FL) model has been widely used in many industries for its high efficiency and confidentiality. Some researchers have explored its confidentiality and designed some algorithms to attack training data sets, but these algorithms all have their own limitations. Therefore, most people still believe that local machine learning gradient information is safe and reliable. In this paper, an algorithm based on gradient features is designed to attack the federated learning model in order to attract more attention to the security of federated learning systems. In federated learning system, gradient contains little information compared with the original training data set, but this project intends to restore the original training image data through gradient information. Convolutional Neural Network (CNN) has excellent performance in image processing. Therefore, the federated learning model of this project is equipped with Convolutional Neural Network structure, and the model is trained by using image data sets. The algorithm calculates the virtual gradient by generating virtual image labels. Then the virtual gradient is matched with the real gradient to restore the original image. This attack algorithm is written in Python language, uses cat and dog classification Kaggle data sets, and gradually extends from the full connection layer to the convolution layer, thus improving the universality. At present, the average squared error between the data recovered by this algorithm and the original image information is approximately 5, and the vast majority of images can be completely restored according to the gradient information given, indicating that the gradient of federated learning system is not absolutely safe and reliable. △ Less\n",
      "Sumbitted: 15/12/2022\n",
      "\n",
      "\n",
      "Document 41:\n",
      "Title: Cats vs Dogs , Photons vs Hadrons\n",
      "Authors: Authors: Francesco Visconti\n",
      "Abstract: In gamma ray astronomy with Cherenkov telescopes, machine learning models are needed to guess what kind of particles generated the detected light, and their energies and directions. The focus in this work is on the classification task, training a simple convolutional neural network suitable for binary classification (as it could be a cats vs dogs classification problem), using as input uncleaned images generated by Montecarlo data for a single ASTRI telescope. Results show an enhanced discriminant power with respect to classical random forest methods. △ Less\n",
      "Sumbitted: 16/12/2022\n",
      "\n",
      "\n",
      "Document 42:\n",
      "Title: Auxiliary Learning as a step towards Artificial General Intelligence\n",
      "Authors: Authors: Christeen T. Jose\n",
      "Abstract: Auxiliary Learning is a machine learning approach in which the model acknowledges the existence of objects that do not come under any of its learned categories.The name Auxiliary learning was chosen due to the introduction of an auxiliary class. The paper focuses on increasing the generality of existing narrow purpose neural networks and also highlights the need to handle unknown objects. The Cat & Dog binary classifier is taken as an example throughout the paper. △ Less\n",
      "Sumbitted: 30/11/2022\n",
      "\n",
      "\n",
      "Document 43:\n",
      "Title: Elliptically-Contoured Tensor-variate Distributions with Application to Improved Image Learning\n",
      "Authors: Authors: Carlos Llosa-Vite , Ranjan Maitra\n",
      "Abstract: Statistical analysis of tensor-valued data has largely used the tensor-variate normal (TVN) distribution that may be inadequate when data comes from distributions with heavier or lighter tails. We study a general family of elliptically contoured (EC) tensor-variate distributions and derive its characterizations, moments, marginal and conditional distributions, and the EC Wishart distribution. We describe procedures for maximum likelihood estimation from data that are (1) uncorrelated draws from an EC distribution, (2) from a scale mixture of the TVN distribution, and (3) from an underlying but unknown EC distribution, where we extend Tyler's robust estimator. A detailed simulation study highlights the benefits of choosing an EC distribution over the TVN for heavier-tailed data. We develop tensor-variate classification rules using discriminant analysis and EC errors and show that they better predict cats and dogs from images in the Animal Faces-HQ dataset than the TVN-based rules. A novel tensor-on-tensor regression and tensor-variate analysis of variance (TANOVA) framework under EC errors is also demonstrated to better characterize gender, age and ethnic origin than the usual TVN-based TANOVA in the celebrated Labeled Faces of the Wild dataset. △ Less\n",
      "Sumbitted: 13/11/2022\n",
      "\n",
      "\n",
      "Document 44:\n",
      "Title: Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories\n",
      "Authors: Authors: Samarth Sinha , Roman Shapovalov , Jeremy Reizenstein , Ignacio Rocco , Natalia Neverova , Andrea Vedaldi , David Novotny\n",
      "Abstract: Obtaining photorealistic reconstructions of objects from sparse views is inherently ambiguous and can only be achieved by learning suitable reconstruction priors. Earlier works on sparse rigid object reconstruction successfully learned such priors from large datasets such as CO3D. In this paper, we extend this approach to dynamic objects. We use cats and dogs as a representative example and introduce Common Pets in 3D (CoP3D), a collection of crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction \"in the wild\". We also propose Tracker-NeRF, a method for learning 4D reconstruction from our dataset. At test time, given a small number of video frames of an unseen object, Tracker-NeRF predicts the trajectories of its 3D points and generates new views, interpolating viewpoint and time. Results on CoP3D reveal significantly better non-rigid new-view synthesis performance than existing baselines. △ Less\n",
      "Sumbitted: 07/11/2022\n",
      "\n",
      "\n",
      "Document 45:\n",
      "Title: An Action Is Worth Multiple Words: Handling Ambiguity in Action Recognition\n",
      "Authors: Authors: Kiyoon Kim , Davide Moltisanti , Oisin Mac Aodha , Laura Sevilla-Lara\n",
      "Abstract: Precisely naming the action depicted in a video can be a challenging and oftentimes ambiguous task. In contrast to object instances represented as nouns (e.g. dog , cat , chair, etc.), in the case of actions, human annotators typically lack a consensus as to what constitutes a specific action (e.g. jogging versus running). In practice, a given video can contain multiple valid positive annotations for the same action. As a result, video datasets often contain significant levels of label noise and overlap between the atomic action classes. In this work, we address the challenge of training multi-label action recognition models from only single positive training labels. We propose two approaches that are based on generating pseudo training examples sampled from similar instances within the train set. Unlike other approaches that use model-derived pseudo-labels, our pseudo-labels come from human annotations and are selected based on feature similarity. To validate our approaches, we create a new evaluation benchmark by manually annotating a subset of EPIC-Kitchens-100's validation set with multiple verb labels. We present results on this new test set along with additional results on a new version of HMDB-51, called Confusing-HMDB-102, where we outperform existing methods in both cases. Data and code are available at https://github.com/kiyoon/verb_ambiguity △ Less\n",
      "Sumbitted: 10/10/2022\n",
      "\n",
      "\n",
      "Document 46:\n",
      "Title: Contrastive Monotonic Pixel-Level Modulation\n",
      "Authors: Authors: Kun Lu , Rongpeng Li , Honggang Zhang\n",
      "Abstract: Continuous one-to-many mapping is a less investigated yet important task in both low-level visions and neural image translation. In this paper, we present a new formulation called MonoPix, an unsupervised and contrastive continuous modulation model, and take a step further to enable a pixel-level spatial control which is critical but can not be properly handled previously. The key feature of this work is to model the monotonicity between controlling signals and the domain discriminator with a novel contrastive modulation framework and corresponding monotonicity constraints. We have also introduced a selective inference strategy with logarithmic approximation complexity and support fast domain adaptations. The state-of-the-art performance is validated on a variety of continuous mapping tasks, including AFHQ cat - dog and Yosemite summer-winter translation. The introduced approach also helps to provide a new solution for many low-level tasks like low-light enhancement and natural noise generation, which is beyond the long-established practice of one-to-one training and inference. Code is available at https://github.com/lukun199/MonoPix. △ Less\n",
      "Sumbitted: 23/07/2022\n",
      "\n",
      "\n",
      "Document 47:\n",
      "Title: EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations\n",
      "Authors: Authors: Min Zhao , Fan Bao , Chongxuan Li , Jun Zhu\n",
      "Abstract: Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I. To this end, we propose energy-guided stochastic differential equations (EGSDE) that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two feature extractors, we carefully design the energy function such that it encourages the transferred image to preserve the domain-independent features and discard domain-specific ones. Further, we provide an alternative explanation of the EGSDE as a product of experts, where each of the three experts (corresponding to the SDE and two feature extractors) solely contributes to faithfulness or realism. Empirically, we compare EGSDE to a large family of baselines on three widely-adopted unpaired I2I tasks under four metrics. EGSDE not only consistently outperforms existing SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the realism results further (e.g., FID of 51.04 in Cat to Dog and FID of 50.43 in Wild to Dog on AFHQ) by tuning hyper-parameters. The code is available at https://github.com/ML-GSAI/EGSDE. △ Less\n",
      "Sumbitted: 20/12/2022\n",
      "\n",
      "\n",
      "Document 48:\n",
      "Title: PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning\n",
      "Authors: Authors: Simone Martone , Francesco Manigrasso , Lamberti Fabrizio , Lia Morra\n",
      "Abstract: Semantic image interpretation can vastly benefit from approaches that combine sub-symbolic distributed representation learning with the capability to reason at a higher level of abstraction. Logic Tensor Networks (LTNs) are a class of neuro-symbolic systems based on a differentiable, first-order logic grounded into a deep neural network. LTNs replace the classical concept of training set with a knowledge base of fuzzy logical axioms. By defining a set of differentiable operators to approximate the role of connectives, predicates, functions and quantifiers, a loss function is automatically specified so that LTNs can learn to satisfy the knowledge base. We focus here on the subsumption or \\texttt{isOfClass} predicate, which is fundamental to encode most semantic image interpretation tasks. Unlike conventional LTNs, which rely on a separate predicate for each class (e.g., dog , cat ), each with its own set of learnable weights, we propose a common \\texttt{isOfClass} predicate, whose level of truth is a function of the distance between an object embedding and the corresponding class prototype. The PROTOtypical Logic Tensor Networks (PROTO-LTN) extend the current formulation by grounding abstract concepts as parametrized class prototypes in a high-dimensional embedding space, while reducing the number of parameters required to ground the knowledge base. We show how this architecture can be effectively trained in the few and zero-shot learning scenarios. Experiments on Generalized Zero Shot Learning benchmarks validate the proposed implementation as a competitive alternative to traditional embedding-based approaches. The proposed formulation opens up new opportunities in zero shot learning settings, as the LTN formalism allows to integrate background knowledge in the form of logical axioms to compensate for the lack of labelled examples. △ Less\n",
      "Sumbitted: 26/06/2022\n",
      "\n",
      "\n",
      "Document 49:\n",
      "Title: Beyond Separability: Analyzing the Linear Transferability of Contrastive Representations to Related Subpopulations\n",
      "Authors: Authors: Jeff Z. HaoChen , Colin Wei , Ananya Kumar , Tengyu Ma\n",
      "Abstract: Contrastive learning is a highly effective method for learning representations from unlabeled data. Recent works show that contrastive representations can transfer across domains, leading to simple state-of-the-art algorithms for unsupervised domain adaptation. In particular, a linear classifier trained to separate the representations on the source domain can also predict classes on the target domain accurately, even though the representations of the two domains are far from each other. We refer to this phenomenon as linear transferability. This paper analyzes when and why contrastive representations exhibit linear transferability in a general unsupervised domain adaptation setting. We prove that linear transferability can occur when data from the same class in different domains (e.g., photo dogs and cartoon dogs ) are more related with each other than data from different classes in different domains (e.g., photo dogs and cartoon cats ) are. Our analyses are in a realistic regime where the source and target domains can have unbounded density ratios and be weakly related, and they have distant representations across domains. △ Less\n",
      "Sumbitted: 23/05/2022\n",
      "\n",
      "\n",
      "Document 50:\n",
      "Title: An Artificial Intelligence Browser Architecture (AIBA) For Our Kind and Others: A Voice Name System Speech implementation with two warrants, Wake Neutrality and Value Preservation of Personally Identifiable Information\n",
      "Authors: Authors: Brian Subirana\n",
      "Abstract: Conversational commerce, first pioneered by Apple's Siri, is the first of may applications based on always-on artificial intelligence systems that decide on its own when to interact with the environment, potentially collecting 24x7 longitudinal training data that is often Personally Identifiable Information (PII). A large body of scholarly papers, on the order of a million according to a simple Google Scholar search, suggests that the treatment of many health conditions, including COVID-19 and dementia, can be vastly improved by this data if the dataset is large enough as it has happened in other domains (e.g. GPT3). In contrast, current dominant systems are closed garden solutions without wake neutrality and that can't fully exploit the PII data they have because of IRB and Cohues-type constraints.\n",
      "  We present a voice browser-and-server architecture that aims to address these two limitations by offering wake neutrality and the possibility to handle PII aiming to maximize its value. We have implemented this browser for the collection of speech samples and have successfully demonstrated it can capture over 200.000 samples of COVID-19 coughs. The architecture we propose is designed so it can grow beyond our kind into other domains such as collecting sound samples from vehicles, video images from nature, ingestible robotics, multi-modal signals (EEG, EKG,...), or even interacting with other kinds such as dogs and cats . △ Less\n",
      "Sumbitted: 31/03/2022\n",
      "\n",
      "\n",
      "Document 51:\n",
      "Title: HINT: Hierarchical Neuron Concept Explainer\n",
      "Authors: Authors: Andong Wang , Wei-Ning Lee , Xiaojuan Qi\n",
      "Abstract: To interpret deep networks, one main approach is to associate neurons with human-understandable concepts. However, existing methods often ignore the inherent relationships of different concepts (e.g., dog and cat both belong to animals), and thus lose the chance to explain neurons responsible for higher-level concepts (e.g., animal). In this paper, we study hierarchical concepts inspired by the hierarchical cognition process of human beings. To this end, we propose HIerarchical Neuron concepT explainer (HINT) to effectively build bidirectional associations between neurons and hierarchical concepts in a low-cost and scalable manner. HINT enables us to systematically and quantitatively study whether and how the implicit hierarchical relationships of concepts are embedded into neurons, such as identifying collaborative neurons responsible to one concept and multimodal neurons for different concepts, at different semantic levels from concrete concepts (e.g., dog ) to more abstract ones (e.g., animal). Finally, we verify the faithfulness of the associations using Weakly Supervised Object Localization, and demonstrate its applicability in various tasks such as discovering saliency regions and explaining adversarial attacks. Code is available on https://github.com/AntonotnaWang/HINT. △ Less\n",
      "Sumbitted: 26/03/2022\n",
      "\n",
      "\n",
      "Document 52:\n",
      "Title: Interpreting Class Conditional GANs with Channel Awareness\n",
      "Authors: Authors: Yingqing He , Zhiyi Zhang , Jiapeng Zhu , Yujun Shen , Qifeng Chen\n",
      "Abstract: Understanding the mechanism of generative adversarial networks (GANs) helps us better use GANs for downstream applications. Existing efforts mainly target interpreting unconditional models, leaving it less explored how a conditional GAN learns to render images regarding various categories. This work fills in this gap by investigating how a class conditional generator unifies the synthesis of multiple classes. For this purpose, we dive into the widely used class-conditional batch normalization (CCBN), and observe that each feature channel is activated at varying degrees given different categorical embeddings. To describe such a phenomenon, we propose channel awareness, which quantitatively characterizes how a single channel contributes to the final synthesis. Extensive evaluations and analyses on the BigGAN model pre-trained on ImageNet reveal that only a subset of channels is primarily responsible for the generation of a particular category, similar categories (e.g., cat and dog ) usually get related to some same channels, and some channels turn out to share information across all classes. For good measure, our algorithm enables several novel applications with conditional GANs. Concretely, we achieve (1) versatile image editing via simply altering a single channel and manage to (2) harmoniously hybridize two different classes. We further verify that the proposed channel awareness shows promising potential in (3) segmenting the synthesized image and (4) evaluating the category-wise synthesis performance. △ Less\n",
      "Sumbitted: 21/03/2022\n",
      "\n",
      "\n",
      "Document 53:\n",
      "Title: A Novel Plug-in Module for Fine-Grained Visual Classification\n",
      "Authors: Authors: Po-Yung Chou , Cheng-Hung Lin , Wen-Chung Kao\n",
      "Abstract: Visual classification can be divided into coarse-grained and fine-grained classification. Coarse-grained classification represents categories with a large degree of dissimilarity, such as the classification of cats and dogs , while fine-grained classification represents classifications with a large degree of similarity, such as cat species, bird species, and the makes or models of vehicles. Unlike coarse-grained visual classification, fine-grained visual classification often requires professional experts to label data, which makes data more expensive. To meet this challenge, many approaches propose to automatically find the most discriminative regions and use local features to provide more precise features. These approaches only require image-level annotations, thereby reducing the cost of annotation. However, most of these methods require two- or multi-stage architectures and cannot be trained end-to-end. Therefore, we propose a novel plug-in module that can be integrated to many common backbones, including CNN-based or Transformer-based networks to provide strongly discriminative regions. The plugin module can output pixel-level feature maps and fuse filtered features to enhance fine-grained visual classification. Experimental results show that the proposed plugin module outperforms state-of-the-art approaches and significantly improves the accuracy to 92.77\\% and 92.83\\% on CUB200-2011 and NABirds, respectively. We have released our source code in Github https://github.com/chou141253/FGVC-PIM.git. △ Less\n",
      "Sumbitted: 08/02/2022\n",
      "\n",
      "\n",
      "Document 54:\n",
      "Title: A simple, sensitive and quantitative FACS-based test for SARS-CoV-2 serology in humans and animals\n",
      "Authors: Authors: Agnès Maurel Ribes , Pierre Bessière , Jean Charles Guéry , Eloïse Joly Featherstone , Timothée Bruel , Remy Robinot , Olivier Schwartz , Romain Volmer , Florence Abravanel , Jacques Izopet , Etienne Joly\n",
      "Abstract: Serological tests are important for understanding the physiopathology and following the evolution of the Covid-19 pandemic. Assays based on flow cytometry (FACS) of tissue culture cells expressing the spike (S) protein of SARS-CoV-2 have repeatedly proven to perform slightly better than the plate-based assays ELISA and CLIA (chemiluminescent immuno-assay), and markedly better than lateral flow immuno-assays (LFIA). Here, we describe an optimized and very simple FACS assay based on staining a mix of two Jurkat cell lines, expressing either high levels of the S protein (Jurkat-S) or a fluorescent protein (Jurkat-R expressing m-Cherry, or Jurkat-G, expressing GFP, which serve as an internal negative control). We show that the Jurkat-S\\&R-flow test has a much broader dynamic range than a commercial ELISA test and performs at least as well in terms of sensitivity and specificity. Also, it is more sensitive and quantitative than the hemagglutination-based test HAT, which we described recently. The Jurkat-flow test requires only a few microliters of blood; thus, it can be used to quantify various Ig isotypes in capillary blood collected from a finger prick. It can be used also to evaluate serological responses in mice, hamsters, cats and dogs . FACS tests offer a very attractive solution for laboratories with access to tissue culture and flow cytometry who want to monitor serological responses in humans or in animals, and how these relate to susceptibility to infection, or re-infection, by the virus, and to protection against Covid-19. △ Less\n",
      "Sumbitted: 14/01/2022\n",
      "\n",
      "\n",
      "Document 55:\n",
      "Title: From Coarse to Fine-grained Concept based Discrimination for Phrase Detection\n",
      "Authors: Authors: Maan Qraitem , Bryan A. Plummer\n",
      "Abstract: Phrase detection requires methods to identify if a phrase is relevant to an image and localize it, if applicable. A key challenge for training more discriminative detection models is sampling negatives. Sampling techniques from prior work focus primarily on hard, often noisy, negatives disregarding the broader distribution of negative samples. Our proposed CFCD-Net addresses this through two novels methods. First, we generate groups of semantically similar words we call concepts (\\eg, \\{ dog , cat , horse\\} and \\ \\{car, truck, SUV\\}), and then train our CFCD-Net to discriminate between a region of interest and its unrelated concepts. Second, for phrases containing fine-grained mutually-exclusive words (\\eg, colors), we force the model to select only one applicable phrase for each region using our novel fine-grained module (FGM). We evaluate our approach on Flickr30K Entities and RefCOCO+, where we improve mAP over the state-of-the-art by 1.5-2 points. When considering only the phrases affected by our FGM module, we improve by 3-4 points on both datasets. △ Less\n",
      "Sumbitted: 14/11/2022\n",
      "\n",
      "\n",
      "Document 56:\n",
      "Title: Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing\n",
      "Authors: Authors: Yuanyuan Yuan , Qi Pang , Shuai Wang\n",
      "Abstract: Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat ). To test DNNs, diverse inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. SOTA works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually valid (e.g., a cat remains a \" cat \" after mutation), existing efforts rely on imprecise and less generalizable heuristics.\n",
      "  This study revisits two key objectives in media input mutation - perception diversity (DIV) and validity (VAL) - in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that DIV and VAL inextricably bound each other, and prove that SOTA generative model-based methods fundamentally fail to mutate real-world media data (either sacrificing DIV or VAL). In contrast, we discuss the feasibility of mutating real-world media data with provably high DIV and VAL based on manifold.\n",
      "  We concretize the technical solution of mutating media data of various formats (images, audios, text) via a unified manner based on manifold. Specifically, when media data are projected into a low-dimensional manifold, the data can be mutated by walking on the manifold with certain directions and step sizes. When contrasted with the input data, the mutated data exhibit encouraging DIV in the perceptual traits (e.g., lying vs. standing dog ) while retaining reasonably high VAL (i.e., a dog remains a dog ). We implement our techniques in DEEPWALK for testing DNNs. DEEPWALK outperforms prior methods in testing comprehensiveness and can find more error-triggering inputs with higher quality. △ Less\n",
      "Sumbitted: 24/10/2023\n",
      "\n",
      "\n",
      "Document 57:\n",
      "Title: Simulation of the behavior of the Leopardus guigna using random walkers\n",
      "Authors: Authors: A. Torres-Hernandez , Byron C. Guzmán , Melanie Kaiser , Julio C. Hernández\n",
      "Abstract: Considering that there is very little information on the behavior habits of guigna cats , as well as investigations in which small populations are captured to place radiocollars on them and then release them in the place where they were captured, which is done with the intention of collecting data on their positions in a territory and thus make estimates of the mean distances they usually travel. Under the hypothesis that guignas maintain a sedentary behavior in a specific area of a given territory, this paper shows one way to simulate a distribution of points in a territory using random walkers to emulate the distribution of the data that would be obtained by placing radiocollars in a population of guignas, with which it is possible to make estimates of the mean distances that move away from a certain fixed position, and the interactions they can have with points in the territory that represent a high probability of lethality, such as farms, packs of dogs , roads, urban areas, etc. It is necessary to mention that by estimating the possible interactions that a guignas population may have with possible predators in a territory with the help of a satellite image, it is possible to evaluate the points of a territory that represent a potentially lethal risk for the guignas, and thus generate relocation strategies that help preserve them. △ Less\n",
      "Sumbitted: 05/11/2021\n",
      "\n",
      "\n",
      "Document 58:\n",
      "Title: Integrating Fréchet distance and AI reveals the evolutionary trajectory and origin of SARS-CoV-2\n",
      "Authors: Authors: Anyou Wang\n",
      "Abstract: A genome, composed of a precisely ordered sequence of four nucleotides (ATCG), encompasses a multitude of specific genome features like AAA motif. Mutations occurring within a genome disrupt the sequential order and composition of these features, thereby influencing the evolutionary trajectories and yielding variants. The evolutionary relatedness between a variant and its ancestor can be estimated by assessing evolutionary distances across a spectrum of genome features. This study develops a novel, alignment-free algorithm that considers both the sequential order and composition of genome features, enabling computation of the Fréchet distance (Fr) across multiple genome features to quantify the evolutionary status of a variant. Integrating this algorithm with an artificial recurrent neural network (RNN) reveals the quantitative evolutionary trajectory and origin of SARS-CoV-2, a puzzle unsolved by alignment-based phylogenetics. The RNN generates the evolutionary trajectory from Fr data at two levels: genome sequence mutations and organism variants. At the genome sequence level, SARS-CoV-2 evolutionarily shortens its genome to enhance its infectious capacity. Mutating signature features, such as TTA and GCT, increases its infectious potential and drives its evolution. At the organism level, variants mutating a single biomarker possess low infectious potential. However, mutating multiple markers dramatically increases their infectious capacity, propelling the COVID-19 pandemic. SARS-CoV-2 likely originates from mink coronavirus variants, with its origin trajectory traced as follows: mink, cat , tiger, mouse, hamster, dog , lion, gorilla, leopard, bat, and pangolin. Together, mutating multiple signature features and biomarkers delineates the evolutionary trajectory of mink-origin SARS-CoV-2, leading to the COVID-19 pandemic. Full text and detailed on https://combai.org/ai/covidgenome/ △ Less\n",
      "Sumbitted: 23/03/2024\n",
      "\n",
      "\n",
      "Document 59:\n",
      "Title: Using Contrastive Learning and Pseudolabels to learn representations for Retail Product Image Classification\n",
      "Authors: Authors: Muktabh Mayank Srivastava\n",
      "Abstract: Retail product Image classification problems are often few shot classification problems, given retail product classes cannot have the type of variations across images like a cat or dog or tree could have. Previous works have shown different methods to finetune Convolutional Neural Networks to achieve better classification accuracy on such datasets. In this work, we try to address the problem statement : Can we pretrain a Convolutional Neural Network backbone which yields good enough representations for retail product images, so that training a simple logistic regression on these representations gives us good classifiers ? We use contrastive learning and pseudolabel based noisy student training to learn representations that get accuracy in order of finetuning the entire Convnet backbone for retail product image classification. △ Less\n",
      "Sumbitted: 07/10/2021\n",
      "\n",
      "\n",
      "Document 60:\n",
      "Title: Generatively Augmented Neural Network Watchdog for Image Classification Networks\n",
      "Authors: Authors: Justin M. Bui , Glauco A. Amigo , Robert J. Marks II\n",
      "Abstract: The identification of out-of-distribution data is vital to the deployment of classification networks. For example, a generic neural network that has been trained to differentiate between images of dogs and cats can only classify an input as either a dog or a cat . If a picture of a car or a kumquat were to be supplied to this classifier, the result would still be either a dog or a cat . In order to mitigate this, techniques such as the neural network watchdog have been developed. The compression of the image input into the latent layer of the autoencoder defines the region of in-distribution in the image space. This in-distribution set of input data has a corresponding boundary in the image space. The watchdog assesses whether inputs are in inside or outside this boundary. This paper demonstrates how to sharpen this boundary using generative network training data augmentation thereby bettering the discrimination and overall performance of the watchdog. △ Less\n",
      "Sumbitted: 07/09/2021\n",
      "\n",
      "\n",
      "Document 61:\n",
      "Title: PHPQ: Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval\n",
      "Authors: Authors: Ziyun Zeng , Jinpeng Wang , Bin Chen , Tao Dai , Shu-Tao Xia , Zhi Wang\n",
      "Abstract: Deep hashing approaches, including deep quantization and deep binary hashing, have become a common solution to large-scale image retrieval due to their high computation and storage efficiency. Most existing hashing methods cannot produce satisfactory results for fine-grained retrieval, because they usually adopt the outputs of the last CNN layer to generate binary codes. Since deeper layers tend to summarize visual clues, e.g., texture, into abstract semantics, e.g., dogs and cats , the feature produced by the last CNN layer is less effective in capturing subtle but discriminative visual details that mostly exist in shallow layers. To improve fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic information from multi-level features, which emphasizes the subtle discrimination of different sub-categories. Besides, we propose a learnable quantization module with a partial codebook attention mechanism, which helps to optimize the most relevant codewords and improves the quantization. Comprehensive experiments on two widely-used public benchmarks, i.e., CUB-200-2011 and Stanford Dogs , demonstrate that PHPQ outperforms state-of-the-art methods. △ Less\n",
      "Sumbitted: 09/01/2024\n",
      "\n",
      "\n",
      "Document 62:\n",
      "Title: Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal\n",
      "Authors: Authors: Lei Zhu , Zhaojing Luo , Wei Wang , Meihui Zhang , Gang Chen , Kaiping Zheng\n",
      "Abstract: Deep learning models usually require a large amount of labeled data to achieve satisfactory performance. In multimedia analysis, domain adaptation studies the problem of cross-domain knowledge transfer from a label rich source domain to a label scarce target domain, thus potentially alleviates the annotation requirement for deep learning models. However, we find that contemporary domain adaptation methods for cross-domain image understanding perform poorly when source domain is noisy. Weakly Supervised Domain Adaptation (WSDA) studies the domain adaptation problem under the scenario where source data can be noisy. Prior methods on WSDA remove noisy source data and align the marginal distribution across domains without considering the fine-grained semantic structure in the embedding space, which have the problem of class misalignment, e.g., features of cats in the target domain might be mapped near features of dogs in the source domain. In this paper, we propose a novel method, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we adopt the cluster assumption and learn cluster discriminatively with class prototypes in the embedding space. We propose to leverage the location information of the data points in the embedding space and model the location information with a Gaussian mixture model to identify noisy source data. We then design a network which incorporates the Gaussian mixture noise model as a sub-module for unsupervised noise removal and propose a novel cluster-level adversarial adaptation method which aligns unlabeled target data with the less noisy class prototypes for mapping the semantic structure across domains. We conduct extensive experiments to evaluate the effectiveness of our method on both general images and medical images from COVID-19 and e-commerce datasets. The results show that our method significantly outperforms state-of-the-art WSDA methods. △ Less\n",
      "Sumbitted: 09/09/2021\n",
      "\n",
      "\n",
      "Document 63:\n",
      "Title: Using Topological Framework for the Design of Activation Function and Model Pruning in Deep Neural Networks\n",
      "Authors: Authors: Yogesh Kochar , Sunil Kumar Vengalil , Neelam Sinha\n",
      "Abstract: Success of deep neural networks in diverse tasks across domains of computer vision, speech recognition and natural language processing, has necessitated understanding the dynamics of training process and also working of trained models. Two independent contributions of this paper are 1) Novel activation function for faster training convergence 2) Systematic pruning of filters of models trained irrespective of activation function. We analyze the topological transformation of the space of training samples as it gets transformed by each successive layer during training, by changing the activation function. The impact of changing activation function on the convergence during training is reported for the task of binary classification. A novel activation function aimed at faster convergence for classification tasks is proposed. Here, Betti numbers are used to quantify topological complexity of data. Results of experiments on popular synthetic binary classification datasets with large Betti numbers(>150) using MLPs are reported. Results show that the proposed activation function results in faster convergence requiring fewer epochs by a factor of 1.5 to 2, since Betti numbers reduce faster across layers with the proposed activation function. The proposed methodology was verified on benchmark image datasets: fashion MNIST, CIFAR-10 and cat -vs- dog images, using CNNs. Based on empirical results, we propose a novel method for pruning a trained model. The trained model was pruned by eliminating filters that transform data to a topological space with large Betti numbers. All filters with Betti numbers greater than 300 were removed from each layer without significant reduction in accuracy. This resulted in faster prediction time and reduced memory size of the model. △ Less\n",
      "Sumbitted: 03/09/2021\n",
      "\n",
      "\n",
      "Document 64:\n",
      "Title: Towards Fine-grained Image Classification with Generative Adversarial Networks and Facial Landmark Detection\n",
      "Authors: Authors: Mahdi Darvish , Mahsa Pouramini , Hamid Bahador\n",
      "Abstract: Fine-grained classification remains a challenging task because distinguishing categories needs learning complex and local differences. Diversity in the pose, scale, and position of objects in an image makes the problem even more difficult. Although the recent Vision Transformer models achieve high performance, they need an extensive volume of input data. To encounter this problem, we made the best use of GAN-based data augmentation to generate extra dataset instances. Oxford-IIIT Pets was our dataset of choice for this experiment. It consists of 37 breeds of cats and dogs with variations in scale, poses, and lighting, which intensifies the difficulty of the classification task. Furthermore, we enhanced the performance of the recent Generative Adversarial Network (GAN), StyleGAN2-ADA model to generate more realistic images while preventing overfitting to the training set. We did this by training a customized version of MobileNetV2 to predict animal facial landmarks; then, we cropped images accordingly. Lastly, we combined the synthetic images with the original dataset and compared our proposed method with standard GANs augmentation and no augmentation with different subsets of training data. We validated our work by evaluating the accuracy of fine-grained image classification on the recent Vision Transformer (ViT) Model. △ Less\n",
      "Sumbitted: 28/08/2021\n",
      "\n",
      "\n",
      "Document 65:\n",
      "Title: Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification\n",
      "Authors: Authors: Hilal AlQuabeh , Ameera Bawazeer , Abdulateef Alhashmi\n",
      "Abstract: Data labeling in supervised learning is considered an expensive and infeasible tool in some conditions. The self-supervised learning method is proposed to tackle the learning effectiveness with fewer labeled data, however, there is a lack of confidence in the size of labeled data needed to achieve adequate results. This study aims to draw a baseline on the proportion of the labeled data that models can appreciate to yield competent accuracy when compared to training with additional labels. The study implements the kaggle.com' cats -vs- dogs dataset, Mnist and Fashion-Mnist to investigate the self-supervised learning task by implementing random rotations augmentation on the original datasets. To reveal the true effectiveness of the pretext process in self-supervised learning, the original dataset is divided into smaller batches, and learning is repeated on each batch with and without the pretext pre-training. Results show that the pretext process in the self-supervised learning improves the accuracy around 15% in the downstream classification task when compared to the plain supervised learning. △ Less\n",
      "Sumbitted: 17/08/2021\n",
      "\n",
      "\n",
      "Document 66:\n",
      "Title: The Randomness of Input Data Spaces is an A Priori Predictor for Generalization\n",
      "Authors: Authors: Martin Briesch , Dominik Sobania , Franz Rothlauf\n",
      "Abstract: Over-parameterized models can perfectly learn various types of data distributions, however, generalization error is usually lower for real data in comparison to artificial data. This suggests that the properties of data distributions have an impact on generalization capability. This work focuses on the search space defined by the input data and assumes that the correlation between labels of neighboring input values influences generalization. If correlation is low, the randomness of the input data space is high leading to high generalization error. We suggest to measure the randomness of an input data space using Maurer's universal. Results for synthetic classification tasks and common image classification benchmarks (MNIST, CIFAR10, and Microsoft's cats vs. dogs data set) find a high correlation between the randomness of input data spaces and the generalization error of deep neural networks for binary classification problems. △ Less\n",
      "Sumbitted: 27/07/2022\n",
      "\n",
      "\n",
      "Document 67:\n",
      "Title: Coarse-to-Fine Curriculum Learning\n",
      "Authors: Authors: Otilia Stretcu , Emmanouil Antonios Platanios , Tom M. Mitchell , Barnabás Póczos\n",
      "Abstract: When faced with learning challenging new tasks, humans often follow sequences of steps that allow them to incrementally build up the necessary skills for performing these new tasks. However, in machine learning, models are most often trained to solve the target tasks directly.Inspired by human learning, we propose a novel curriculum learning approach which decomposes challenging tasks into sequences of easier intermediate goals that are used to pre-train a model before tackling the target task. We focus on classification tasks, and design the intermediate tasks using an automatically constructed label hierarchy. We train the model at each level of the hierarchy, from coarse labels to fine labels, transferring acquired knowledge across these levels. For instance, the model will first learn to distinguish animals from objects, and then use this acquired knowledge when learning to classify among more fine-grained classes such as cat , dog , car, and truck. Most existing curriculum learning algorithms for supervised learning consist of scheduling the order in which the training examples are presented to the model. In contrast, our approach focuses on the output space of the model. We evaluate our method on several established datasets and show significant performance gains especially on classification problems with many labels. We also evaluate on a new synthetic dataset which allows us to study multiple aspects of our method. △ Less\n",
      "Sumbitted: 07/06/2021\n",
      "\n",
      "\n",
      "Document 68:\n",
      "Title: My cat Chester's dynamical systems analysyyyyy7777777777777777y7is of the laser pointer and the red dot on the wall: correlation, causation, or SARS-Cov-2 hallucination?\n",
      "Authors: Authors: Eve Armstrong , Chester\n",
      "Abstract: My cat Chester investigates the elusive relationship between the appearance in my hand of a silver laser pointer and that of a red dot on the wall, or on the floor, or on any other object that resides within the vicinity of the laser pointer. Chester first assesses preliminary establishments for causality, including mutual information, temporal precedence, and control for third variables. These assessments are all inconclusive for various reasons. In particular, mutual information fails to illuminate the problem due to a dearth of information regarding what the laser pointer might have been doing at times following Chester's first awareness of the dot. Next Chester performs a formal reconstruction of phase space via time-delay embedding, to unfold the gggggggggggfffgfgtredvteometry ,mmmm.........,.,,......,.mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm of the underlying dynamical system giving rise to the red dot's trajectory. The resulting attractor does not resemble a laser pointer. The reconstruction could, however, be flawed, for example, due to the short temporal duration of the dot's observed trajectory. Finally, the red dot could be a hallucination: a symptom brought on by COVID-19 - because, well, these days pretty much anything might be a symptom brought on by COVID-19. On this note, Chester's kitten brother Mad Dog Lapynski offers an independent check on the red dot's existence. Moreover, the results of this study are inconclusive and ca[pokilki[[[[[ll for follow-up. △ Less\n",
      "Sumbitted: 03/04/2021\n",
      "\n",
      "\n",
      "Document 69:\n",
      "Title: Exploiting Class Similarity for Machine Learning with Confidence Labels and Projective Loss Functions\n",
      "Authors: Authors: Gautam Rajendrakumar Gare , John Michael Galeotti\n",
      "Abstract: Class labels used for machine learning are relatable to each other, with certain class labels being more similar to each other than others (e.g. images of cats and dogs are more similar to each other than those of cats and cars). Such similarity among classes is often the cause of poor model performance due to the models confusing between them. Current labeling techniques fail to explicitly capture such similarity information. In this paper, we instead exploit the similarity between classes by capturing the similarity information with our novel confidence labels. Confidence labels are probabilistic labels denoting the likelihood of similarity, or confusability, between the classes. Often even after models are trained to differentiate between classes in the feature space, the similar classes' latent space still remains clustered. We view this type of clustering as valuable information and exploit it with our novel projective loss functions. Our projective loss functions are designed to work with confidence labels with an ability to relax the loss penalty for errors that confuse similar classes. We use our approach to train neural networks with noisy labels, as we believe noisy labels are partly a result of confusability arising from class similarity. We show improved performance compared to the use of standard loss functions. We conduct a detailed analysis using the CIFAR-10 dataset and show our proposed methods' applicability to larger datasets, such as ImageNet and Food-101N. △ Less\n",
      "Sumbitted: 25/03/2021\n",
      "\n",
      "\n",
      "Document 70:\n",
      "Title: Pinch-off dynamics to describe animal lapping\n",
      "Authors: Authors: Sunghwan Jung\n",
      "Abstract: Some carnivorous mammals (e.g., cats and dogs ) lap water with their tongues to drink water at high frequencies. Such a fast moving tongue creates a liquid column out of a bath which is bitten by the mouth for drinking. Presumably, the animals bite just before the pinch-off time of the water column to maximize the water intake. Otherwise, the water column falls back to the bath before being bitten. Such a pinch-off phenomenon in the liquid column can be described as the acceleration-induced (i.e., unsteady) inertia balances with the capillary force. The classical Rayleigh-Plateau instability explains the competition of the steady inertia with the capillarity, but not with the unsteady inertia. In this study, we modify the Rayleigh-Plateau instability in the presence of the fluid acceleration, and show that the most unstable wavenumber and growth rate increase with acceleration. The pinch-off time is theoretically predicted as the -1/3 power of the Bond number (i.e, a ratio of the acceleration-induced inertia to capillarity). Finally, measured pinch-off times from previous physical experiments and dog \\& cat jaw-closing times are shown to be in good agreement with our theoretical pinch-off time. Therefore, our study shows that animals presumably modulate their lapping and jaw-closing time to bite down the water column before the pinch-off to maximize the water intake. △ Less\n",
      "Sumbitted: 11/03/2021\n",
      "\n",
      "\n",
      "Document 71:\n",
      "Title: Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing\n",
      "Authors: Authors: Christian Reimers , Paul Bodesheim , Jakob Runge , Joachim Denzler\n",
      "Abstract: Bias in classifiers is a severe issue of modern deep learning methods, especially for their application in safety- and security-critical areas. Often, the bias of a classifier is a direct consequence of a bias in the training dataset, frequently caused by the co-occurrence of relevant features and irrelevant ones. To mitigate this issue, we require learning algorithms that prevent the propagation of bias from the dataset into the classifier. We present a novel adversarial debiasing method, which addresses a feature that is spuriously connected to the labels of training images but statistically independent of the labels for test images. Thus, the automatic identification of relevant features during training is perturbed by irrelevant features. This is the case in a wide range of bias-related problems for many computer vision tasks, such as automatic skin cancer detection or driver assistance. We argue by a mathematical proof that our approach is superior to existing techniques for the abovementioned bias. Our experiments show that our approach performs better than state-of-the-art techniques on a well-known benchmark dataset with real-world images of cats and dogs . △ Less\n",
      "Sumbitted: 10/03/2021\n",
      "\n",
      "\n",
      "Document 73:\n",
      "Title: Equal Affection or Random Selection: the Quality of Subjective Feedback from a Group Perspective\n",
      "Authors: Authors: Jiale Chen , Yuqing Kong , Yuxuan Lu\n",
      "Abstract: In the setting where a group of agents is asked a single subjective multi-choice question (e.g. which one do you prefer? cat or dog ?), we are interested in evaluating the quality of the collected feedback. However, the collected statistics are not sufficient to reflect how informative the feedback is since fully informative feedback (equal affection of the choices) and fully uninformative feedback (random selection) have the same uniform statistics.\n",
      "  Here we distinguish the above two scenarios by additionally asking for respondents' predictions about others' choices. We assume that informative respondents' predictions strongly depend on their own choices while uninformative respondents' do not. With this assumption, we propose a new definition for uninformative feedback and correspondingly design a family of evaluation metrics, called f-variety, for group-level feedback which can 1) distinguish informative feedback and uninformative feedback (separation) even if their statistics are both uniform and 2) decrease as the ratio of uninformative respondents increases (monotonicity). We validate our approach both theoretically and numerically. Moreover, we conduct two real-world case studies about 1) comparisons about athletes and 2) comparisons about stand-up comedians to show the superiority of our approach. △ Less\n",
      "Sumbitted: 24/02/2021\n",
      "\n",
      "\n",
      "Document 74:\n",
      "Title: Multi-Interest-Aware User Modeling for Large-Scale Sequential Recommendations\n",
      "Authors: Authors: Jianxun Lian , Iyad Batal , Zheng Liu , Akshay Soni , Eun Yong Kang , Yajun Wang , Xing Xie\n",
      "Abstract: Precise user modeling is critical for online personalized recommendation services. Generally, users' interests are diverse and are not limited to a single aspect, which is particularly evident when their behaviors are observed for a longer time. For example, a user may demonstrate interests in cats / dogs , dancing and food \\& delights when browsing short videos on Tik Tok; the same user may show interests in real estate and women's wear in her web browsing behaviors. Traditional models tend to encode a user's behaviors into a single embedding vector, which do not have enough capacity to effectively capture her diverse interests.\n",
      "  This paper proposes a Sequential User Matrix (SUM) to accurately and efficiently capture users' diverse interests. SUM models user behavior with a multi-channel network, with each channel representing a different aspect of the user's interests. User states in different channels are updated by an \\emph{erase-and-add} paradigm with interest- and instance-level attention. We further propose a local proximity debuff component and a highway connection component to make the model more robust and accurate. SUM can be maintained and updated incrementally, making it feasible to be deployed for large-scale online serving. We conduct extensive experiments on two datasets. Results demonstrate that SUM consistently outperforms state-of-the-art baselines. △ Less\n",
      "Sumbitted: 18/05/2021\n",
      "\n",
      "\n",
      "Document 75:\n",
      "Title: Detection of data drift and outliers affecting machine learning model performance over time\n",
      "Authors: Authors: Samuel Ackerman , Eitan Farchi , Orna Raz , Marcel Zalmanovici , Parijat Dube\n",
      "Abstract: A trained ML model is deployed on another `test' dataset where target feature values (labels) are unknown. Drift is distribution change between the training and deployment data, which is concerning if model performance changes. For a cat / dog image classifier, for instance, drift during deployment could be rabbit images (new class) or cat / dog images with changed characteristics (change in distribution). We wish to detect these changes but can't measure accuracy without deployment data labels. We instead detect drift indirectly by nonparametrically testing the distribution of model prediction confidence for changes. This generalizes our method and sidesteps domain-specific feature representation.\n",
      "  We address important statistical issues, particularly Type-1 error control in sequential testing, using Change Point Models (CPMs; see Adams and Ross 2012). We also use nonparametric outlier methods to show the user suspicious observations for model diagnosis, since the before/after change confidence distributions overlap significantly. In experiments to demonstrate robustness, we train on a subset of MNIST digit classes, then insert drift (e.g., unseen digit class) in deployment data in various settings (gradual/sudden changes in the drift proportion). A novel loss function is introduced to compare the performance (detection delay, Type-1 and 2 errors) of a drift detector under different levels of drift class contamination. △ Less\n",
      "Sumbitted: 06/09/2022\n",
      "\n",
      "\n",
      "Document 76:\n",
      "Title: Are DNNs fooled by extremely unrecognizable images?\n",
      "Authors: Authors: Soichiro Kumano , Hiroshi Kera , Toshihiko Yamasaki\n",
      "Abstract: Fooling images are a potential threat to deep neural networks (DNNs). These images are not recognizable to humans as natural objects, such as dogs and cats , but are misclassified by DNNs as natural-object classes with high confidence scores. Despite their original design concept, existing fooling images retain some features that are characteristic of the target objects if looked into closely. Hence, DNNs can react to these features. In this paper, we address the question of whether there can be fooling images with no characteristic pattern of natural objects locally or globally. As a minimal case, we introduce single-color images with a few pixels altered, called sparse fooling images (SFIs). We first prove that SFIs always exist under mild conditions for linear and nonlinear models and reveal that complex models are more likely to be vulnerable to SFI attacks. With two SFI generation methods, we demonstrate that in deeper layers, SFIs end up with similar features to those of natural images, and consequently, fool DNNs successfully. Among other layers, we discovered that the max pooling layer causes the vulnerability against SFIs. The defense against SFIs and transferability are also discussed. This study highlights the new vulnerability of DNNs by introducing a novel class of images that distributes extremely far from natural images. △ Less\n",
      "Sumbitted: 26/03/2022\n",
      "\n",
      "\n",
      "Document 77:\n",
      "Title: Beyond Cats and Dogs : Semi-supervised Classification of fuzzy labels with overclustering\n",
      "Authors: Authors: Lars Schmarje , Johannes Brünger , Monty Santarossa , Simon-Martin Schröder , Rainer Kiko , Reinhard Koch\n",
      "Abstract: A long-standing issue with deep learning is the need for large and consistently labeled datasets. Although the current research in semi-supervised learning can decrease the required amount of annotated data by a factor of 10 or even more, this line of research still uses distinct classes like cats and dogs . However, in the real-world we often encounter problems where different experts have different opinions, thus producing fuzzy labels. We propose a novel framework for handling semi-supervised classifications of such fuzzy labels. Our framework is based on the idea of overclustering to detect substructures in these fuzzy labels. We propose a novel loss to improve the overclustering capability of our framework and show on the common image classification dataset STL-10 that it is faster and has better overclustering performance than previous work. On a real-world plankton dataset, we illustrate the benefit of overclustering for fuzzy labels and show that we beat previous state-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more consistent predictions of substructures. △ Less\n",
      "Sumbitted: 19/10/2021\n",
      "\n",
      "\n",
      "Document 78:\n",
      "Title: CaCL: Class-aware Codebook Learning for Weakly Supervised Segmentation on Diffuse Image Patterns\n",
      "Authors: Authors: Ruining Deng , Quan Liu , Shunxing Bao , Aadarsh Jha , Catie Chang , Bryan A. Millis , Matthew J. Tyska , Yuankai Huo\n",
      "Abstract: Weakly supervised learning has been rapidly advanced in biomedical image analysis to achieve pixel-wise labels (segmentation) from image-wise annotations (classification), as biomedical images naturally contain image-wise labels in many scenarios. The current weakly supervised learning algorithms from the computer vision community are largely designed for focal objects (e.g., dogs and cats ). However, such algorithms are not optimized for diffuse patterns in biomedical imaging (e.g., stains and fluorescence in microscopy imaging). In this paper, we propose a novel class-aware codebook learning (CaCL) algorithm to perform weakly supervised learning for diffuse image patterns. Specifically, the CaCL algorithm is deployed to segment protein expressed brush border regions from histological images of human duodenum. Our contribution is three-fold: (1) we approach the weakly supervised segmentation from a novel codebook learning perspective; (2) the CaCL algorithm segments diffuse image patterns rather than focal objects; and (3) the proposed algorithm is implemented in a multi-task framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) via joint image reconstruction, classification, feature embedding, and segmentation. The experimental results show that our method achieved superior performance compared with baseline weakly supervised algorithms. The code is available at https://github.com/ddrrnn123/CaCL. △ Less\n",
      "Sumbitted: 13/04/2022\n",
      "\n",
      "\n",
      "Document 79:\n",
      "Title: Are mouse and cat the missing link in the COVID-19 outbreaks in seafood markets?\n",
      "Authors: Authors: Daniel H. Tao , Weitao Sun\n",
      "Abstract: Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus caused the novel coronavirus disease-2019 (COVID-19) affecting the whole world. Like SARS-CoV and MERS-CoV, SARS-CoV-2 are thought to originate in bats and then spread to humans through intermediate hosts. Identifying intermediate host species is critical to understanding the evolution and transmission mechanisms of COVID-19. However, determining which animals are intermediate hosts remains a key challenge. Virus host-genome similarity (HGS) is an important factor that reflects the adaptability of virus to host. SARS-CoV-2 may retain beneficial mutations to increase HGS and evade the host immune system. This study investigated the HGSs between 399 SARS-CoV-2 strains and 10 hosts of different species, including bat, mouse, cat , swine, snake, dog , pangolin, chicken, human and monkey. The results showed that the HGS between SARS-CoV-2 and bat was the highest, followed by mouse and cat . Human and monkey had the lowest HGS values. In terms of genetic similarity, mouse and monkey are halfway between bat and human. Moreover, given that COVID-19 outbreaks tend to be associated with live poultry and seafood markets, mouse and cat are more likely sources of infection in these places. However, more experimental data are needed to confirm whether mouse and cat are true intermediate hosts. These findings suggest that animals closely related to human life, especially those with high HGS, need to be closely monitored. △ Less\n",
      "Sumbitted: 18/09/2020\n",
      "\n",
      "\n",
      "Document 80:\n",
      "Title: Distributional Generalization: A New Kind of Generalization\n",
      "Authors: Authors: Preetum Nakkiran , Yamini Bansal\n",
      "Abstract: We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers. △ Less\n",
      "Sumbitted: 14/10/2020\n",
      "\n",
      "\n",
      "Document 82:\n",
      "Title: Estimating semantic structure for the VQA answer space\n",
      "Authors: Authors: Corentin Kervadec , Grigory Antipov , Moez Baccouche , Christian Wolf\n",
      "Abstract: Since its appearance, Visual Question Answering (VQA, i.e. answering a question posed over an image), has always been treated as a classification problem over a set of predefined answers. Despite its convenience, this classification approach poorly reflects the semantics of the problem limiting the answering to a choice between independent proposals, without taking into account the similarity between them (e.g. equally penalizing for answering cat or German shepherd instead of dog ). We address this issue by proposing (1) two measures of proximity between VQA classes, and (2) a corresponding loss which takes into account the estimated proximity. This significantly improves the generalization of VQA models by reducing their language bias. In particular, we show that our approach is completely model-agnostic since it allows consistent improvements with three different VQA models. Finally, by combining our method with a language bias reduction approach, we report SOTA-level performance on the challenging VQAv2-CP dataset. △ Less\n",
      "Sumbitted: 08/04/2021\n",
      "\n",
      "\n",
      "Document 83:\n",
      "Title: Information Mandala: Statistical Distance Matrix with Clustering\n",
      "Authors: Authors: Xin Lu\n",
      "Abstract: In machine learning, observation features are measured in a metric space to obtain their distance function for optimization. Given similar features that are statistically sufficient as a population, a statistical distance between two probability distributions can be calculated for more precise learning. Provided the observed features are multi-valued, the statistical distance function is still efficient. However, due to its scalar output, it cannot be applied to represent detailed distances between feature elements. To resolve this problem, this paper extends the traditional statistical distance to a matrix form, called a statistical distance matrix. In experiments, the proposed approach performs well in object recognition tasks and clearly and intuitively represents the dissimilarities between cat and dog images in the CIFAR dataset, even when directly calculated using the image pixels. By using the hierarchical clustering of the statistical distance matrix, the image pixels can be separated into several clusters that are geometrically arranged around a center like a Mandala pattern. The statistical distance matrix with clustering, called the Information Mandala, is beyond ordinary saliency maps and can help to understand the basic principles of the convolution neural network. △ Less\n",
      "Sumbitted: 22/06/2020\n",
      "\n",
      "\n",
      "Document 84:\n",
      "Title: RGBD- Dog : Predicting Canine Pose from RGBD Sensors\n",
      "Authors: Authors: Sinead Kearney , Wenbin Li , Martin Parsons , Kwang In Kim , Darren Cosker\n",
      "Abstract: The automatic extraction of animal \\reb{3D} pose from images without markers is of interest in a range of scientific fields. Most work to date predicts animal pose from RGB images, based on 2D labelling of joint positions. However, due to the difficult nature of obtaining training data, no ground truth dataset of 3D animal motion is available to quantitatively evaluate these approaches. In addition, a lack of 3D animal pose data also makes it difficult to train 3D pose-prediction methods in a similar manner to the popular field of body-pose prediction. In our work, we focus on the problem of 3D canine pose estimation from RGBD images, recording a diverse range of dog breeds with several Microsoft Kinect v2s, simultaneously obtaining the 3D ground truth skeleton via a motion capture system. We generate a dataset of synthetic RGBD images from this data. A stacked hourglass network is trained to predict 3D joint locations, which is then constrained using prior models of shape and pose. We evaluate our model on both synthetic and real RGBD images and compare our results to previously published work fitting canine models to images. Finally, despite our training set consisting only of dog data, visual inspection implies that our network can produce good predictions for images of other quadrupeds -- e.g. horses or cats -- when their pose is similar to that contained in our training set. △ Less\n",
      "Sumbitted: 16/04/2020\n",
      "\n",
      "\n",
      "Document 85:\n",
      "Title: What did we learn from forty years of research on semantic interference? A Bayesian metaanalysis\n",
      "Authors: Authors: A. Bürki , S. Elbuy , S. Madec , S. Vasishth\n",
      "Abstract: When participants in an experiment have to name pictures while ignoring distractor words superimposed on the picture or presented auditorily (i.e., picture-word interference paradigm), they take more time when the word to be named (or target) and distractor words are from the same semantic category (e.g., cat - dog ). This experimental effect is known as the semantic interference effect, and is probably one of the most studied in the language production literature. The functional origin of the effect and the exact conditions in which it occurs are however still debated. Since Lupker reported the effect in the first response time experiment about 40 years ago, more than 300 similar experiments have been conducted. The semantic interference effect was replicated in many experiments, but several studies also reported the absence of an effect in a subset of experimental conditions. The aim of the present study is to provide a comprehensive theoretical review of the existing evidence to date and several Bayesian meta-analyses and meta-regressions to determine the size of the effect and explore the experimental conditions in which the effect surfaces. The results are discussed in the light of current debates about the functional origin of the semantic interference effect and its implications for our understanding of the language production system. △ Less\n",
      "Sumbitted: 26/04/2020\n",
      "\n",
      "\n",
      "Document 86:\n",
      "Title: A Comparative Study for Non-rigid Image Registration and Rigid Image Registration\n",
      "Authors: Authors: Xiaoran Zhang , Hexiang Dong , Di Gao , Xiao Zhao\n",
      "Abstract: Image registration algorithms can be generally categorized into two groups: non-rigid and rigid. Recently, many deep learning-based algorithms employ a neural net to characterize non-rigid image registration function. However, do they always perform better? In this study, we compare the state-of-art deep learning-based non-rigid registration approach with rigid registration approach. The data is generated from Kaggle Dog vs Cat Competition \\url{https://www.kaggle.com/c/ dogs -vs- cats /} and we test the algorithms' performance on rigid transformation including translation, rotation, scaling, shearing and pixelwise non-rigid transformation. The Voxelmorph is trained on rigidset and nonrigidset separately for comparison and we also add a gaussian blur layer to its original architecture to improve registration performance. The best quantitative results in both root-mean-square error (RMSE) and mean absolute error (MAE) metrics for rigid registration are produced by SimpleElastix and non-rigid registration by Voxelmorph. We select representative samples for visual assessment. △ Less\n",
      "Sumbitted: 11/01/2020\n",
      "\n",
      "\n",
      "Document 87:\n",
      "Title: Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers\n",
      "Authors: Authors: Colin Paterson , Radu Calinescu , Chiara Picardi\n",
      "Abstract: Regions of high-dimensional input spaces that are underrepresented in training datasets reduce machine-learnt classifier performance, and may lead to corner cases and unwanted bias for classifiers used in decision making systems. When these regions belong to otherwise well-represented classes, their presence and negative impact are very hard to identify. We propose an approach for the detection and mitigation of such rare subclasses in deep neural network classifiers. The new approach is underpinned by an easy-to-compute commonality metric that supports the detection of rare subclasses, and comprises methods for reducing the impact of these subclasses during both model training and model exploitation. We demonstrate our approach using two well-known datasets, MNIST's handwritten digits and Kaggle's cats / dogs , identifying rare subclasses and producing models which compensate for subclass rarity. In addition we demonstrate how our run-time approach increases the ability of users to identify samples likely to be misclassified at run-time. △ Less\n",
      "Sumbitted: 07/07/2021\n",
      "\n",
      "\n",
      "Document 88:\n",
      "Title: Visual Privacy Protection via Mapping Distortion\n",
      "Authors: Authors: Yiming Li , Peidong Liu , Yong Jiang , Shu-Tao Xia\n",
      "Abstract: Privacy protection is an important research area, which is especially critical in this big data era. To a large extent, the privacy of visual classification data is mainly in the mapping between the image and its corresponding label, since this relation provides a great amount of information and can be used in other scenarios. In this paper, we propose the mapping distortion based protection (MDP) and its augmentation-based extension (AugMDP) to protect the data privacy by modifying the original dataset. In the modified dataset generated by MDP, the image and its label are not consistent ($e.g.$, a cat -like image is labeled as the dog ), whereas the DNNs trained on it can still achieve good performance on benign testing set. As such, this method can protect privacy when the dataset is leaked. Extensive experiments are conducted, which verify the effectiveness and feasibility of our method. The code for reproducing main results is available at \\url{https://github.com/PerdonLiu/Visual-Privacy-Protection-via-Mapping-Distortion}. △ Less\n",
      "Sumbitted: 03/02/2021\n",
      "\n",
      "\n",
      "Document 89:\n",
      "Title: Learning to Disentangle Robust and Vulnerable Features for Adversarial Detection\n",
      "Authors: Authors: Byunggill Joe , Sung Ju Hwang , Insik Shin\n",
      "Abstract: Although deep neural networks have shown promising performances on various tasks, even achieving human-level performance on some, they are shown to be susceptible to incorrect predictions even with imperceptibly small perturbations to an input. There exists a large number of previous works which proposed to defend against such adversarial attacks either by robust inference or detection of adversarial inputs. Yet, most of them cannot effectively defend against whitebox attacks where an adversary has a knowledge of the model and defense. More importantly, they do not provide a convincing reason why the generated adversarial inputs successfully fool the target models. To address these shortcomings of the existing approaches, we hypothesize that the adversarial inputs are tied to latent features that are susceptible to adversarial perturbation, which we call vulnerable features. Then based on this intuition, we propose a minimax game formulation to disentangle the latent features of each instance into robust and vulnerable ones, using variational autoencoders with two latent spaces. We thoroughly validate our model for both blackbox and whitebox attacks on MNIST, Fashion MNIST5, and Cat & Dog datasets, whose results show that the adversarial inputs cannot bypass our detector without changing its semantics, in which case the attack has failed. △ Less\n",
      "Sumbitted: 10/09/2019\n",
      "\n",
      "\n",
      "Document 91:\n",
      "Title: Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\n",
      "Authors: Authors: He He , Sheng Zha , Haohan Wang\n",
      "Abstract: Statistical natural language inference (NLI) models are susceptible to learning dataset bias: superficial cues that happen to associate with the label on a particular dataset, but are not useful in general, e.g., negation words indicate contradiction. As exposed by several recent challenge datasets, these models perform poorly when such association is absent, e.g., predicting that \"I love dogs \" contradicts \"I don't love cats \". Our goal is to design learning algorithms that guard against known dataset bias. We formalize the concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual fitting, which we call DRiFt. We first learn a biased model that only uses features that are known to relate to dataset bias. Then, we train a debiased model that fits to the residual of the biased model, focusing on examples that cannot be predicted well by biased features only. We use DRiFt to train three high-performing NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve significant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets. △ Less\n",
      "Sumbitted: 24/11/2019\n",
      "\n",
      "\n",
      "Document 92:\n",
      "Title: Pareto-optimal data compression for binary classification tasks\n",
      "Authors: Authors: Max Tegmark , Tailin Wu\n",
      "Abstract: The goal of lossy data compression is to reduce the storage cost of a data set $X$ while retaining as much information as possible about something ($Y$) that you care about. For example, what aspects of an image $X$ contain the most information about whether it depicts a cat ? Mathematically, this corresponds to finding a mapping $X\\to Z\\equiv f(X)$ that maximizes the mutual information $I(Z,Y)$ while the entropy $H(Z)$ is kept below some fixed threshold. We present a method for mapping out the Pareto frontier for classification tasks, reflecting the tradeoff between retained entropy and class information. We first show how a random variable $X$ (an image, say) drawn from a class $Y\\in\\{1,...,n\\}$ can be distilled into a vector $W=f(X)\\in \\mathbb{R}^{n-1}$ losslessly, so that $I(W,Y)=I(X,Y)$; for example, for a binary classification task of cats and dogs , each image $X$ is mapped into a single real number $W$ retaining all information that helps distinguish cats from dogs . For the $n=2$ case of binary classification, we then show how $W$ can be further compressed into a discrete variable $Z=g_β(W)\\in\\{1,...,m_β\\}$ by binning $W$ into $m_β$ bins, in such a way that varying the parameter $β$ sweeps out the full Pareto frontier, solving a generalization of the Discrete Information Bottleneck (DIB) problem. We argue that the most interesting points on this frontier are \"corners\" maximizing $I(Z,Y)$ for a fixed number of bins $m=2,3...$ which can be conveniently be found without multiobjective optimization. We apply this method to the CIFAR-10, MNIST and Fashion-MNIST datasets, illustrating how it can be interpreted as an information-theoretically optimal image clustering algorithm. △ Less\n",
      "Sumbitted: 15/01/2020\n",
      "\n",
      "\n",
      "Document 93:\n",
      "Title: SHREWD: Semantic Hierarchy-based Relational Embeddings for Weakly-supervised Deep Hashing\n",
      "Authors: Authors: Heikki Arponen , Tom E Bishop\n",
      "Abstract: Using class labels to represent class similarity is a typical approach to training deep hashing systems for retrieval; samples from the same or different classes take binary 1 or 0 similarity values. This similarity does not model the full rich knowledge of semantic relations that may be present between data points. In this work we build upon the idea of using semantic hierarchies to form distance metrics between all available sample labels; for example cat to dog has a smaller distance than cat to guitar. We combine this type of semantic distance into a loss function to promote similar distances between the deep neural network embeddings. We also introduce an empirical Kullback-Leibler divergence loss term to promote binarization and uniformity of the embeddings. We test the resulting SHREWD method and demonstrate improvements in hierarchical retrieval scores using compact, binary hash codes instead of real valued ones, and show that in a weakly supervised hashing setting we are able to learn competitively without explicitly relying on class labels, but instead on similarities between labels. △ Less\n",
      "Sumbitted: 12/08/2019\n",
      "\n",
      "\n",
      "Document 94:\n",
      "Title: DCT-CompCNN: A Novel Image Classification Network Using JPEG Compressed DCT Coefficients\n",
      "Authors: Authors: Bulla Rajesh , Mohammed Javed , Ratnesh , Shubham Srivastava\n",
      "Abstract: The popularity of Convolutional Neural Network (CNN) in the field of Image Processing and Computer Vision has motivated researchers and industrialist experts across the globe to solve different challenges with high accuracy. The simplest way to train a CNN classifier is to directly feed the original RGB pixels images into the network. However, if we intend to classify images directly with its compressed data, the same approach may not work better, like in case of JPEG compressed images. This research paper investigates the issues of modifying the input representation of the JPEG compressed data, and then feeding into the CNN. The architecture is termed as DCT-CompCNN. This novel approach has shown that CNNs can also be trained with JPEG compressed DCT coefficients, and subsequently can produce a better performance in comparison with the conventional CNN approach. The efficiency of the modified input representation is tested with the existing ResNet-50 architecture and the proposed DCT-CompCNN architecture on a public image classification datasets like Dog Vs Cat and CIFAR-10 datasets, reporting a better performance △ Less\n",
      "Sumbitted: 26/07/2019\n",
      "\n",
      "\n",
      "Document 95:\n",
      "Title: Cross-Platform Modeling of Users' Behavior on Social Media\n",
      "Authors: Authors: Haiqian Gu , Jie Wang , Ziwen Wang , Bojin Zhuang , Wenhao Bian , Fei Su\n",
      "Abstract: With the booming development and popularity of mobile applications, different verticals accumulate abundant data of user information and social behavior, which are spontaneous, genuine and diversified. However, each platform describes user's portraits in only certain aspect, resulting in difficult combination of those internet footprints together. In our research, we proposed a modeling approach to analyze user's online behavior across different social media platforms. Structured and unstructured data of same users shared by NetEase Music and Sina Weibo have been collected for cross-platform analysis of correlations between music preference and other users' characteristics. Based on music tags of genre and mood, genre cluster of five groups and mood cluster of four groups have been formed by computing their collected song lists with K-means method. Moreover, with the help of user data of Weibo, correlations between music preference (i.e. genre, mood) and Big Five personalities (BFPs) and basic information (e.g. gender, resident region, tags) have been comprehensively studied, building up full-scale user portraits with finer grain. Our findings indicate that people's music preference could be linked with their real social activities. For instance, people living in mountainous areas generally prefer folk music, while those in urban areas like pop music more. Interestingly, dog lovers could love sad music more than cat lovers. Moreover, our proposed cross-platform modeling approach could be adapted to other verticals, providing an online automatic way for profiling users in a more precise and comprehensive way. △ Less\n",
      "Sumbitted: 23/06/2019\n",
      "\n",
      "\n",
      "Document 96:\n",
      "Title: Hierarchical Auxiliary Learning\n",
      "Authors: Authors: Jaehoon Cha , Kyeong Soo Kim , Sanghyuk Lee\n",
      "Abstract: Conventional application of convolutional neural networks (CNNs) for image classification and recognition is based on the assumption that all target classes are equal(i.e., no hierarchy) and exclusive of one another (i.e., no overlap). CNN-based image classifiers built on this assumption, therefore, cannot take into account an innate hierarchy among target classes (e.g., cats and dogs in animal image classification) or additional information that can be easily derived from the data (e.g.,numbers larger than five in the recognition of handwritten digits), thereby resulting in scalability issues when the number of target classes is large. Combining two related but slightly different ideas of hierarchical classification and logical learning by auxiliary inputs, we propose a new learning framework called hierarchical auxiliary learning, which not only address the scalability issues with a large number of classes but also could further reduce the classification/recognition errors with a reasonable number of classes. In the hierarchical auxiliary learning, target classes are semantically or non-semantically grouped into superclasses, which turns the original problem of mapping between an image and its target class into a new problem of mapping between a pair of an image and its superclass and the target class. To take the advantage of superclasses, we introduce an auxiliary block into a neural network, which generates auxiliary scores used as additional information for final classification/recognition; in this paper, we add the auxiliary block between the last residual block and the fully-connected output layer of the ResNet. Experimental results demonstrate that the proposed hierarchical auxiliary learning can reduce classification errors up to 0.56, 1.6 and 3.56 percent with MNIST, SVHN and CIFAR-10 datasets, respectively. △ Less\n",
      "Sumbitted: 03/06/2019\n",
      "\n",
      "\n",
      "Document 98:\n",
      "Title: A Two-Step Learning Method For Detecting Landmarks on Faces From Different Domains\n",
      "Authors: Authors: Bruna Vieira Frade , Erickson R. Nascimento\n",
      "Abstract: The detection of fiducial points on faces has significantly been favored by the rapid progress in the field of machine learning, in particular in the convolution networks. However, the accuracy of most of the detectors strongly depends on an enormous amount of annotated data. In this work, we present a domain adaptation approach based on a two-step learning to detect fiducial points on human and animal faces. We evaluate our method on three different datasets composed of different animal faces ( cats , dogs , and horses). The experiments show that our method performs better than state of the art and can use few annotated data to leverage the detection of landmarks reducing the demand for large volume of annotated data. △ Less\n",
      "Sumbitted: 12/09/2018\n",
      "\n",
      "\n",
      "Document 99:\n",
      "Title: Improving Shape Deformation in Unsupervised Image-to-Image Translation\n",
      "Authors: Authors: Aaron Gokaslan , Vivek Ramanujan , Daniel Ritchie , Kwang In Kim , James Tompkin\n",
      "Abstract: Unsupervised image-to-image translation techniques are able to map local texture between two domains, but they are typically unsuccessful when the domains require larger shape change. Inspired by semantic segmentation, we introduce a discriminator with dilated convolutions that is able to use information from across the entire image to train a more context-aware generator. This is coupled with a multi-scale perceptual loss that is better able to represent error in the underlying shape of objects. We demonstrate that this design is more capable of representing shape deformation in a challenging toy dataset, plus in complex mappings with significant dataset variation between humans, dolls, and anime faces, and between cats and dogs . △ Less\n",
      "Sumbitted: 17/01/2019\n",
      "\n",
      "\n",
      "Document 100:\n",
      "Title: Hair histology as a tool for forensic identification of some domestic animal species\n",
      "Authors: Authors: Yasser A. Ahmed , Safwat Ali , Ahmed Ghallab\n",
      "Abstract: Animal hair examination at a criminal scene may provide valuable information in forensic investigations. However, local reference databases for animal hair identification are rare. In the present study, we provide differential histological analysis of hair of some domestic animals in Upper Egypt. For this purpose, guard hair of large ruminants (buffalo, camel and cow), small ruminants (sheep and goat), equine (horse and donkey) and canine ( dog and cat ) were collected and comparative analysis was performed by light microscopy. Based on the hair cuticle scale pattern, type and diameter of the medulla, and the pigmentation, characteristic differential features of each animal species were identified. The cuticle scale pattern was imbricate in all tested animals except in donkey, in which coronal scales were identified. The cuticle scale margin type, shape and the distance in between were characteristic for each animal species. The hair medulla was continuous in most of the tested animal species with the exception of sheep, in which fragmental medulla was detected. The diameter of the hair medulla and the margins differ according to the animal species. Hair shaft pigmentation were not detected in all tested animals with the exception of camel and buffalo, in which granules and streak-like pigmentation were detected. In conclusion, the present study provides a first-step towards preparation of a complete local reference database for animal hair identification that can be used in forensic investigations. △ Less\n",
      "Sumbitted: 07/07/2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from datetime import datetime\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Λήψη NLTK πόρων (εφόσον δεν είναι ήδη κατεβασμένα)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Αρχικοποίηση εργαλείων NLP\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Συνάρτηση επεξεργασίας κειμένου (tokenization, stemming/lemmatization, stop-word removal and special characters removal)\n",
    "def preprocess_text(text):\n",
    "    # Χωρίζει το κείμενο σε λέξεις μέσω της word_tokenize\n",
    "    tokens = word_tokenize(text) \n",
    "    # Εφαρμόζει lemmatization σε κάθε λέξη\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens] \n",
    "    # Φιλτράρει λέξεις, αφαιρώντας stopwords και special characters\n",
    "    filtered_tokens = [token.lower().strip() for token in lemmatized_tokens if token.lower().strip() not in stop_words and token not in string.punctuation and token]\n",
    "    # Ενώνει τις φιλτραρισμένες λέξεις σε ένα string\n",
    "    processed_text = ''.join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def rank_documents(documents, query, similarity_threshold=0):\n",
    "    # Εισαγωγή απαραίτητης βιβλιοθήκης για TF-IDF\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # Δημιουργία ενός vectorizer TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    # Υπολογισμός των TF-IDF τιμών για τα έγγραφα\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    # Μετατροπή του ερωτήματος σε TF-IDF διάνυσμα\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    # Κατάταξη των εγγράφων με βάση τις τιμές TF-IDF σε σχέση με το ερώτημα\n",
    "    scores = tfidf_matrix.dot(query_vector.T).toarray().flatten()\n",
    "    # Φιλτράρισμα των εγγράφων με βάση το κατώφλι ομοιότητας\n",
    "    results = [(documents[i], scores[i]) for i in range(len(documents)) if scores[i] > similarity_threshold]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Αλγόριθμος Boolean retrieval\n",
    "def boolean_retrieval(query, inverted_index, documents):\n",
    "    # Διαχωρισμός των λέξεων του ερωτήματος\n",
    "    query_terms = query.split()\n",
    "    # Εύρεση σχετικών εγγράφων που περιέχουν όλους τους όρους του ερωτήματος\n",
    "    relevant_docs = set.intersection(*(set(inverted_index.get(term, set())) for term in query_terms))\n",
    "    return list(relevant_docs)\n",
    "\n",
    "# Εφαρμογή λογικού τελεστή ανάμεσα σε δύο σύνολα\n",
    "def apply_logical_operator(operand1, operand2, operator):\n",
    "    if operator == 'AND':\n",
    "        return set.intersection(operand1, operand2)\n",
    "    elif operator == 'OR':\n",
    "        return set.union(operand1, operand2)\n",
    "    elif operator == 'NOT':\n",
    "        return set.difference(operand1, operand2)\n",
    "\n",
    "# Επεξεργασία ενός σύνθετου ερωτήματος με λογικούς τελεστές\n",
    "def process_query(query, inverted_index, documents):\n",
    "    # Διαχωρισμός των όρων του ερωτήματος\n",
    "    query_terms = query.split()\n",
    "    # Λίστα για την αποθήκευση των ενδιάμεσων αποτελεσμάτων\n",
    "    processed_query = []\n",
    "    logical_operator = None # Μεταβλητή για τον τρέχοντα λογικό τελεστή\n",
    "    \n",
    "    for term in query_terms:\n",
    "        # Αν ο όρος είναι λογικός τελεστής, τον αποθηκεύουμε\n",
    "        if term.upper() in ('AND', 'OR', 'NOT'):\n",
    "            logical_operator = term.upper()\n",
    "        else:\n",
    "            # Εφαρμογή του λογικού τελεστή στα ενδιάμεσα αποτελέσματα\n",
    "            if logical_operator and processed_query:\n",
    "                processed_query[-1] = apply_logical_operator(processed_query[-1], set(boolean_retrieval(term, inverted_index, documents)), logical_operator)\n",
    "                logical_operator = None\n",
    "            # Αναζήτηση εγγράφων για τον τρέχοντα όρο      \n",
    "            else:\n",
    "                processed_query.append(set(boolean_retrieval(term, inverted_index, documents)))\n",
    "                \n",
    "    # Επεξεργασία του τελεστή NOT στη λίστα\n",
    "    while 'NOT' in processed_query:\n",
    "        not_index = processed_query.index('NOT')\n",
    "        processed_query[not_index:not_index + 2] = apply_logical_operator(set(range(len(documents))), processed_query[not_index + 1], 'NOT')\n",
    "    return processed_query\n",
    "\n",
    "search = input(\"What Are You Looking For?\")\n",
    "search = search.replace(' ', '-')\n",
    "url = f\"https://arxiv.org/search/?searchtype=all&query={search}&abstracts=show&size=100&order=-announced_date_first\"\n",
    "page = requests.get(url).text\n",
    "documents = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "# Βρίσκουμε το τμήμα HTML που περιέχει τα έγγραφα, αναζητώντας τη λίστα με την κλάση \"breathe-horizontal\"\n",
    "div = documents.find(\"ol\", {\"class\": \"breathe-horizontal\"})\n",
    "\n",
    "# Δημιουργούμε μια λίστα για να αποθηκεύσουμε τις πληροφορίες κάθε εγγράφου\n",
    "documents_list = []\n",
    "# Διατρέχουμε κάθε στοιχείο \"li\" στη λίστα εγγράφων\n",
    "for doc in div.find_all(\"li\", recursive=False):\n",
    "    # Εξαγωγή και καθαρισμός του τίτλου\n",
    "    title = doc.find(class_=\"title is-5 mathjax\").get_text(strip=True, separator=' ')\n",
    "    # Εξαγωγή και καθαρισμός των συγγραφέων\n",
    "    authors = doc.find(class_=\"authors\").get_text(strip=True, separator=' ')\n",
    "    # Εξαγωγή και καθαρισμός της περίληψης\n",
    "    abstract = doc.find(\"span\", {\"class\": \"abstract-full has-text-grey-dark mathjax\"}).get_text(strip=True, separator=' ')\n",
    "    # Εξαγωγή και καθαρισμός πρόσθετων πληροφοριών, όπως η ημερομηνία\n",
    "    additional_info = doc.find(\"p\", {\"class\": \"is-size-7\"}).get_text(strip=True, separator=' ')\n",
    "    date_text = additional_info.split(';')[0].strip().replace(\"Submitted\",\"\")\n",
    "    original_date_object = datetime.strptime(date_text, \" %d %B, %Y\")\n",
    "    formatted_date = original_date_object.strftime(\"%d/%m/%Y\")\n",
    "    \n",
    "    # Δημιουργία ενός λεξικού με τις πληροφορίες του εγγράφου\n",
    "    document_info = {\n",
    "        \"Title\": title,\n",
    "        \"Authors\": authors,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Sumbitted\": formatted_date\n",
    "    }\n",
    "\n",
    "    # Προσθέτουμε το λεξικό στη λίστα εγγράφων\n",
    "    documents_list.append(document_info)\n",
    "\n",
    "# Αποθήκευση των πληροφοριών εγγράφων σε αρχείο κειμένου\n",
    "with open(\"arxivdocstest.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for idx, document_info in enumerate(documents_list, start=1):\n",
    "        file.write(f\"Document {idx}:\\n\")\n",
    "        for key, value in document_info.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "        file.write('='*50 + '\\n')\n",
    " \n",
    "\n",
    "# Κατεβάζουμε δεδομένα NLTK αν δεν είναι ήδη εγκατεστημένα\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Κατακερματίζουμε το κείμενο (tokenization) για κάθε έγγραφο στη λίστα\n",
    "tokenized_documents_list = []\n",
    "\n",
    "for document_info in documents_list:\n",
    "    tokenized_document_info = {}\n",
    "\n",
    "    for key, value in document_info.items():\n",
    "        # Χρήση της word_tokenize για κατακερματισμό του κειμένου\n",
    "        tokens = word_tokenize(value)\n",
    "        \n",
    "        # Αποθήκευση των tokens στο λεξικό\n",
    "        tokenized_document_info[key] = tokens\n",
    "\n",
    "    # Προσθέτουμε το tokenized λεξικό στη λίστα\n",
    "    tokenized_documents_list.append(tokenized_document_info)\n",
    "\n",
    "# Αποθήκευση των tokenized πληροφοριών σε αρχείο κειμένου\n",
    "with open(\"tokenized_arxivdocs.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for idx, tokenized_document_info in enumerate(tokenized_documents_list, start=1):\n",
    "        file.write(f\"Tokenized Document {idx}:\\n\")\n",
    "        for key, tokens in tokenized_document_info.items():\n",
    "            file.write(f\"{key}: {tokens}\\n\")\n",
    "        file.write('='*50 + '\\n')\n",
    "\n",
    "        \n",
    "# Επεξεργασία κάθε εγγράφου στη λίστα tokenized εγγράφων\n",
    "preprocessed_documents_list = []\n",
    "\n",
    "for tokenized_document_info in tokenized_documents_list:\n",
    "    preprocessed_document_info = {}\n",
    "    \n",
    "    for key, tokens in tokenized_document_info.items():\n",
    "        # Επεξεργασία των tokens και φιλτράρισμα κενών strings\n",
    "        processed_tokens = [preprocess_text(token) for token in tokens if preprocess_text(token)]\n",
    "        # Αποθήκευση των επεξεργασμένων tokens στο λεξικό\n",
    "        preprocessed_document_info[key] = processed_tokens\n",
    "\n",
    "    # Προσθέτουμε το επεξεργασμένο λεξικό στη λίστα\n",
    "    preprocessed_documents_list.append(preprocessed_document_info)\n",
    "\n",
    "# Αποθήκευση των επεξεργασμένων πληροφοριών σε αρχείο κειμένου\n",
    "with open(\"preprocessed_arxivdocs.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for idx, preprocessed_document_info in enumerate(preprocessed_documents_list, start=1):\n",
    "        file.write(f\"Preprocessed Document {idx}:\\n\")\n",
    "        for key, processed_tokens in preprocessed_document_info.items():\n",
    "            file.write(f\"{key}: {processed_tokens}\\n\")\n",
    "        file.write('='*50 + '\\n')\n",
    "        \n",
    "        \n",
    "# Ανεστραμμένη δομή δεδομένων ευρετηρίου (inverted index)\n",
    "inverted_index = {}\n",
    "\n",
    "for idx, preprocessed_document_info in enumerate(preprocessed_documents_list, start=1):\n",
    "    for key, processed_tokens in preprocessed_document_info.items():\n",
    "        for token in processed_tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(idx)\n",
    "\n",
    "\n",
    "# Εξαγωγή και αποθήκευση του ανεστραμμένου ευρετηρίου (inverted index) σε αρχείο κειμένου\n",
    "with open(\"inverted_index.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for term, document_ids in inverted_index.items():\n",
    "        file.write(f\"{term}: {document_ids}\\n\")\n",
    "\n",
    "# Άνοιγμα του αρχείου \"arxivdocstest.txt\" που περιέχει τις αποθηκευμένες πληροφορίες εγγράφων\n",
    "with open('arxivdocstest.txt', encoding=\"utf8\") as arxivdocstest:\n",
    "    sent1 = arxivdocstest.read()\n",
    "    sent1sp = re.split(r'={50,}', sent1)\n",
    "    \n",
    "    \n",
    "# Ζητάμε από τον χρήστη να εισάγει το ερώτημα και τι αλγόριθμος θέλει να χρησιμοποιηθεί\n",
    "query = input(\"Enter The Query: \")\n",
    "alg = input(\"\\nWhat Type Of Retrieval You Want To Use? \\n1)Boolean Retrieval \\n2)Vector Space Model \\n3)Okapi BM25\\n\")\n",
    "# Boolean Retrieval\n",
    "if alg == \"1\":\n",
    "        # Επεξεργασία του ερωτήματος με χρήση του Boolean Retrieval αλγορίθμου\n",
    "        result = process_query(query, inverted_index, documents)\n",
    "        print(\"\\nRetrieved Results:\")\n",
    "        res_list = []\n",
    "        for doc_set in result:\n",
    "            for doc_id in doc_set:\n",
    "                print(sent1sp[doc_id-1])\n",
    "                res=sent1sp[doc_id-1]\n",
    "                res_list.append(res)\n",
    "# Vector Space Model (VSM)          \n",
    "elif alg == \"2\":\n",
    "    # Υπολογισμός της ομοιότητας και κατάταξη των εγγράφων\n",
    "    results = rank_documents(sent1sp, query)\n",
    "    # Εκτύπωση των ταξινομημένων εγγράφων με βάση την ομοιότητα\n",
    "    for doc, similarity in results:\n",
    "        print(f\"Similarity: {similarity:.2f}\\n{doc}\\n\")\n",
    "# Okapi BM25             \n",
    "elif alg == \"3\":\n",
    "    # Προετοιμασία δεδομένων για BM25\n",
    "    # Tokenize τα έγγραφα\n",
    "    tokenized_corpus = [word_tokenize(doc.lower()) for doc in sent1sp]\n",
    "    # Δημιουργία αντικειμένου BM25\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    # Tokenize το query\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    # Υπολογισμός βαθμολογιών συνάφειας\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    # Ταξινόμηση εγγράφων με βάση τις βαθμολογίες\n",
    "    ranked_results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    # Εμφάνιση αποτελεσμάτων\n",
    "    print(\"\\nRetrieved Results (Ranked):\")\n",
    "    for idx, score in ranked_results:\n",
    "        if score > 0:  # Εμφάνιση μόνο σχετικών εγγράφων\n",
    "            print(f\"Score: {score:.2f}\\nDocument: {sent1sp[idx]}\\n\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Invalid algorithm selection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761985bb-b99a-4506-ac29-bf4fbacf0f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
